{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "c9e18423",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 13px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\">\n",
                "This notebook documents my theoretical study alongside the lab exercises conducted on <b>July 8, 2025</b>.\n",
                "</h5>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6f6d3398",
            "metadata": {},
            "source": [
                "### <u><b>LAB EXERCISES:</b></u> **WEEK 5**"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "95b99a58",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<b style=\"font-size: 16px;\">Overview:</b> This week focuses on building an <b>expert knowledge worker</b> using <b>Retrieval-Augmented Generation (RAG)</b> to accurately answer questions for an insurance tech company. The labs guide you through implementing a simple RAG pipeline that retrieves relevant information from a knowledge base to <b>ground LLM responses</b>, with a strong emphasis on <b>practical, low-cost deployment</b> for enterprise applications.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3fb77fc8",
            "metadata": {},
            "source": [
                "#### <code>**day1.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "78a4b156",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<b style=\"font-size: 16px;\">Abstract:</b> Build a simple <b>RAG (Retrieval-Augmented Generation)</b> pipeline to create an expert knowledge worker for <b>Insurellm</b>. Load context from files and answer questions with high accuracy. Focus: Low-cost, brute-force retrieval and grounding LLM responses in company-specific data.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "50afa607",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<h4 style=\"margin-bottom: 0.4em;\"><b>Expert Knowledge Worker</b></h4>\n",
                "\n",
                "<b>Overview:</b><br>\n",
                "A question-answering agent designed as an <b>expert knowledge worker</b> to assist employees at <b>Insurellm</b>, an Insurance Tech company. This system prioritizes <b>accuracy</b> while maintaining a <b>low-cost</b> deployment.<br>\n",
                "\n",
                "<b>What is RAG?</b><br>\n",
                "<b>Retrieval-Augmented Generation (RAG)</b> is an AI architecture that enhances LLM outputs by retrieving relevant information from external sources (e.g., documents or databases) and injecting it into the model's prompt. This improves factual accuracy and reduces hallucinations.<br>\n",
                "\n",
                "<b>Approach:</b><br>\n",
                "The solution leverages <b>RAG</b> to improve response accuracy by grounding answers in retrieved knowledge. The initial implementation adopts a <b>simple brute-force RAG</b> mechanism to demonstrate feasibility and performance.<br>\n",
                "\n",
                "<b style=\"font-size: 13.5px;\">Sidenote: Business Application Relevance</b><br>\n",
                "RAG is arguably the most practical technique covered in this course. Many commercial applications already use similar pipelines to perform <b>context-aware retrieval over large document stores</b> — such as insurance contracts, financial policies, or product specs. It offers a <b>quick-to-market, cost-effective</b> strategy to enhance LLM utility in enterprise environments.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "76e00908",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run in Anaconda Prompt (for conda users):\n",
                "# conda install -c conda-forge python-dotenv gradio openai langchain langchain-community langchain-openai langchain-chroma scikit-learn plotly sentence-transformers langchain-huggingface faiss-cpu matplotlib\n",
                "\n",
                "# pip users:\n",
                "# pip install python-dotenv gradio openai langchain langchain-community langchain-openai langchain-chroma scikit-learn plotly sentence-transformers langchain-huggingface faiss-cpu matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b5ad546b",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "from dotenv import load_dotenv\n",
                "import gradio as gr\n",
                "from openai import OpenAI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "80abacd7",
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL = 'gpt-4o-mini'\n",
                "# MODEL_LLAMA_HF = 'meta-llama/Meta-Llama-3.1-8B-Instruct'  \n",
                "# MODEL_GEMINI = 'gemini-1.5-flash'  \n",
                "# MODEL_LLAMA_LOCAL = 'llama3.2'  \n",
                "\n",
                "# Example: \n",
                "# To use local Llama3.2 with OpenAI-compatible API (Ollama), use:\n",
                "# openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "082f1fd7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load environment variables in a file called .env\n",
                "\n",
                "load_dotenv(override=True)\n",
                "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
                "openai = OpenAI()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7c67f0a8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# With massive thanks to student Dr John S. for fixing a bug in the below for Windows users!\n",
                "\n",
                "context = {}\n",
                "\n",
                "employees = glob.glob(\"knowledge-base/employees/*\")\n",
                "\n",
                "for employee in employees:\n",
                "    name = employee.split(' ')[-1][:-3]\n",
                "    doc = \"\"\n",
                "    with open(employee, \"r\", encoding=\"utf-8\") as f:\n",
                "        doc = f.read()\n",
                "    context[name]=doc"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0b434e50",
            "metadata": {},
            "outputs": [],
            "source": [
                "context[\"Lancaster\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "33cdd539",
            "metadata": {},
            "outputs": [],
            "source": [
                "products = glob.glob(\"knowledge-base/products/*\")\n",
                "\n",
                "for product in products:\n",
                "    name = product.split(os.sep)[-1][:-3]\n",
                "    doc = \"\"\n",
                "    with open(product, \"r\", encoding=\"utf-8\") as f:\n",
                "        doc = f.read()\n",
                "    context[name]=doc"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8da87379",
            "metadata": {},
            "outputs": [],
            "source": [
                "context.keys()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8391549b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# system_message = \"You are an expert in answering accurate questions about Insurellm, the Insurance Tech company. Give brief, accurate answers. If you don't know the answer, say so. Do not make anything up if you haven't been provided with relevant context.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fdca8a29",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_relevant_context(message):\n",
                "    relevant_context = []\n",
                "    for context_title, context_details in context.items():\n",
                "        if context_title.lower() in message.lower():\n",
                "            relevant_context.append(context_details)\n",
                "    return relevant_context          "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "55a67541",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_relevant_context(\"Who is lancaster?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9d114272",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_relevant_context(\"Who is Avery and what is carllm?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "38e79157",
            "metadata": {},
            "outputs": [],
            "source": [
                "def add_context(message):\n",
                "    relevant_context = get_relevant_context(message)\n",
                "    if relevant_context:\n",
                "        message += \"\\n\\nThe following additional context might be relevant in answering this question:\\n\\n\"\n",
                "        for relevant in relevant_context:\n",
                "            message += relevant + \"\\n\\n\"\n",
                "    return message"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ec734d27",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(add_context(\"Who is Alex Lancaster?\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0edec77",
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat(message, history):\n",
                "    messages = [{\"role\": \"system\", \"content\": system_message}] + history\n",
                "    message = add_context(message)\n",
                "    messages.append({\"role\": \"user\", \"content\": message})\n",
                "\n",
                "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
                "\n",
                "    response = \"\"\n",
                "    for chunk in stream:\n",
                "        response += chunk.choices[0].delta.content or ''\n",
                "        yield response"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3f965c31",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<h4 style=\"margin-bottom: 0.4em;\"><b>Now we will bring this up in Gradio using the Chat interface</b></h4>\n",
                "A quick and easy way to prototype a chat with an LLM.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ead63e84",
            "metadata": {},
            "outputs": [],
            "source": [
                "view = gr.ChatInterface(chat, type=\"messages\").launch()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d5905f11",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Questions to Test</b></h5>\n",
                "<ul style=\"margin: 0.4em 0; padding-left: 1.5em;\">\n",
                "  <li>Who is Alex Lancaster?</li>\n",
                "  <li>What is the CarLLM product?</li>\n",
                "  <li>Who are the employees in the Insurellm knowledge base?</li>\n",
                "  <li>What does the Insurellm company do?</li>\n",
                "  <li>Tell me about Avery's role at Insurellm.</li>\n",
                "  <li>What insurance products does Insurellm offer?</li>\n",
                "  <li>Who is responsible for product development?</li>\n",
                "  <li>What is the main feature of the CarLLM product?</li>\n",
                "  <li>Who can I contact for claims support?</li>\n",
                "  <li>List all products mentioned in the knowledge base.</li>\n",
                "</ul>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c57c4d4f",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f98e5348",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d2b589de",
            "metadata": {},
            "source": [
                "#### <code>**day2.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6040bb61",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<b style=\"font-size: 16px;\">Abstract:</b> Automate document loading and chunking from the knowledge base. Focus: Text preprocessing and chunking, preparing data for vector storage in the next step.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "191c9def",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "from dotenv import load_dotenv\n",
                "import gradio as gr"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a6eeec37",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
                "from langchain.text_splitter import CharacterTextSplitter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "79a39803",
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL = \"gpt-4o-mini\"\n",
                "db_name = \"vector_db\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "27fad618",
            "metadata": {},
            "outputs": [],
            "source": [
                "load_dotenv(override=True)\n",
                "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "64ded5e6",
            "metadata": {},
            "outputs": [],
            "source": [
                "folders = glob.glob(\"knowledge-base/*\")\n",
                "\n",
                "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
                "text_loader_kwargs = {'encoding': 'utf-8'}\n",
                "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
                "# text_loader_kwargs={'autodetect_encoding': True}\n",
                "\n",
                "documents = []\n",
                "for folder in folders:\n",
                "    doc_type = os.path.basename(folder)\n",
                "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
                "    folder_docs = loader.load()\n",
                "    for doc in folder_docs:\n",
                "        doc.metadata[\"doc_type\"] = doc_type\n",
                "        documents.append(doc)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "98318bef",
            "metadata": {},
            "outputs": [],
            "source": [
                "len(documents)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "75b8fdce",
            "metadata": {},
            "outputs": [],
            "source": [
                "documents[24]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ff2d6846",
            "metadata": {},
            "outputs": [],
            "source": [
                "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "chunks = text_splitter.split_documents(documents)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f1ffd0e4",
            "metadata": {},
            "outputs": [],
            "source": [
                "len(chunks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "65540030",
            "metadata": {},
            "outputs": [],
            "source": [
                "chunks[6]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d64d6ef5",
            "metadata": {},
            "outputs": [],
            "source": [
                "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
                "print(f\"Document types found: {', '.join(doc_types)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1e504703",
            "metadata": {},
            "outputs": [],
            "source": [
                "for chunk in chunks:\n",
                "    if 'CEO' in chunk.page_content:\n",
                "        print(chunk)\n",
                "        print(\"_________\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4e3ea9f3",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2e03ecaa",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "29035f1a",
            "metadata": {},
            "source": [
                "#### <code>**day3.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "06c40639",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<b style=\"font-size: 16px;\">Abstract:</b> Generate vector embeddings for each document chunk using either <b>OpenAI</b> or <b>HuggingFace</b> models. Store them in a <b>Chroma</b> vector database. Visualize the vector space in 2D and 3D to gain insight into how your knowledge is represented.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6416c3a4",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "from dotenv import load_dotenv\n",
                "import gradio as gr"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3098b65d",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
                "from langchain.text_splitter import CharacterTextSplitter\n",
                "from langchain.schema import Document\n",
                "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
                "from langchain_chroma import Chroma\n",
                "import numpy as np\n",
                "from sklearn.manifold import TSNE\n",
                "import plotly.graph_objects as go"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4b4d04f2",
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL = \"gpt-4o-mini\"\n",
                "db_name = \"vector_db\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f371b149",
            "metadata": {},
            "outputs": [],
            "source": [
                "load_dotenv(override=True)\n",
                "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2687dc3d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read in documents using LangChain's loaders\n",
                "# Take everything in all the sub-folders of our knowledgebase\n",
                "\n",
                "folders = glob.glob(\"knowledge-base/*\")\n",
                "\n",
                "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
                "text_loader_kwargs = {'encoding': 'utf-8'}\n",
                "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
                "# text_loader_kwargs={'autodetect_encoding': True}\n",
                "\n",
                "documents = []\n",
                "for folder in folders:\n",
                "    doc_type = os.path.basename(folder)\n",
                "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
                "    folder_docs = loader.load()\n",
                "    for doc in folder_docs:\n",
                "        doc.metadata[\"doc_type\"] = doc_type\n",
                "        documents.append(doc)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ff7e7a18",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<b style=\"font-size: 16px;\">⚠️ Please Note:</b><br>\n",
                "In the next cell, we split the text into chunks.<br>\n",
                "Two students reported that the operation caused their computers to crash. They resolved the issue by adjusting the chunking parameters:<br>\n",
                "<ul style=\"margin: 0.5em 0; padding-left: 1.5em;\">\n",
                "  <li><code>chunk_size</code>: from <code>1000</code> to <code>2000</code></li>\n",
                "  <li><code>chunk_overlap</code>: from <code>200</code> to <code>400</code></li>\n",
                "</ul>\n",
                "This change should not be necessary in most cases, but if you encounter similar issues, feel free to apply it.<br>\n",
                "<span style=\"color: gray;\"><i>Note:</i> LangChain may issue a warning about chunk sizes exceeding 1000 — this can be safely ignored.</span><br>\n",
                "<i>Special thanks to Steven W and Nir P for reporting and resolving this issue 🙏</i>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd74e015",
            "metadata": {},
            "outputs": [],
            "source": [
                "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "chunks = text_splitter.split_documents(documents)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cb6ea215",
            "metadata": {},
            "outputs": [],
            "source": [
                "len(chunks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71ac0fda",
            "metadata": {},
            "outputs": [],
            "source": [
                "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
                "print(f\"Document types found: {', '.join(doc_types)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c0f55f7b",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em; font-size: 16px;\"><b>Sidenote on Embeddings and Auto-Encoding LLMs</b></h5>\n",
                "\n",
                "We will be mapping each chunk of text into a vector that represents its meaning — this is called an <b>embedding</b>.<br>\n",
                "\n",
                "To do this, we will use OpenAI’s embedding model via API calls wrapped in LangChain code. This model is an example of an <b>Auto-Encoding LLM</b>, which processes an entire input to generate a fixed output. It differs from <b>Auto-Regressive LLMs</b> (like GPT), which generate outputs token by token based only on prior context.<br>\n",
                "\n",
                "One well-known Auto-Encoding model is <b>BERT</b> from Google. In addition to generating embeddings, these models are also commonly used for classification tasks.<br>\n",
                "\n",
                "<b>Sidenote:</b><br>\n",
                "In <b>Week 8</b>, we’ll return to RAG and vector embeddings using an <b>open-source vector encoder</b> so that data processing remains completely local. This is a crucial consideration for enterprise applications where data privacy and internal compliance are essential.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "96f72bf9",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<b>Option 1:</b> Use <code>embeddings = OpenAIEmbeddings()</code><br>\n",
                "This sets up the embedding model that will convert text chunks into vector representations using OpenAI’s API.\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0b7e050",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
                "\n",
                "embeddings = OpenAIEmbeddings()\n",
                "\n",
                "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
                "# Then replace embeddings = OpenAIEmbeddings()\n",
                "# with:\n",
                "# from langchain.embeddings import HuggingFaceEmbeddings\n",
                "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "03378453",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if a Chroma Datastore already exists - if so, delete the collection to start from scratch\n",
                "\n",
                "if os.path.exists(db_name):\n",
                "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a3f32d5d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create our Chroma vectorstore!\n",
                "\n",
                "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
                "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "35c73582",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get one vector and find how many dimensions it has\n",
                "\n",
                "collection = vectorstore._collection\n",
                "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
                "dimensions = len(sample_embedding)\n",
                "print(f\"The vectors have {dimensions:,} dimensions\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "76ee3ed5",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em; font-size: 16px;\"><b>Visualizing the Vector Store</b></h5>\n",
                "Let’s take a moment to examine the documents and their corresponding embedding vectors to better understand how the system is organizing and retrieving information.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "270347c7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prework\n",
                "\n",
                "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
                "vectors = np.array(result['embeddings'])\n",
                "documents = result['documents']\n",
                "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
                "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9693837d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# We humans find it easier to visalize things in 2D!\n",
                "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
                "# (t-distributed stochastic neighbor embedding)\n",
                "\n",
                "tsne = TSNE(n_components=2, random_state=42)\n",
                "reduced_vectors = tsne.fit_transform(vectors)\n",
                "\n",
                "# Create the 2D scatter plot\n",
                "fig = go.Figure(data=[go.Scatter(\n",
                "    x=reduced_vectors[:, 0],\n",
                "    y=reduced_vectors[:, 1],\n",
                "    mode='markers',\n",
                "    marker=dict(size=5, color=colors, opacity=0.8),\n",
                "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
                "    hoverinfo='text'\n",
                ")])\n",
                "\n",
                "fig.update_layout(\n",
                "    title='2D Chroma Vector Store Visualization',\n",
                "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
                "    width=800,\n",
                "    height=600,\n",
                "    margin=dict(r=20, b=10, l=10, t=40)\n",
                ")\n",
                "\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7a511522",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's try 3D!\n",
                "\n",
                "tsne = TSNE(n_components=3, random_state=42)\n",
                "reduced_vectors = tsne.fit_transform(vectors)\n",
                "\n",
                "# Create the 3D scatter plot\n",
                "fig = go.Figure(data=[go.Scatter3d(\n",
                "    x=reduced_vectors[:, 0],\n",
                "    y=reduced_vectors[:, 1],\n",
                "    z=reduced_vectors[:, 2],\n",
                "    mode='markers',\n",
                "    marker=dict(size=5, color=colors, opacity=0.8),\n",
                "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
                "    hoverinfo='text'\n",
                ")])\n",
                "\n",
                "fig.update_layout(\n",
                "    title='3D Chroma Vector Store Visualization',\n",
                "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
                "    width=900,\n",
                "    height=700,\n",
                "    margin=dict(r=20, b=10, l=10, t=40)\n",
                ")\n",
                "\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1e31c6cb",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<b>Option 2:</b> Use <code>embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")</code><br>\n",
                "This sets up the embedding model using Hugging Face's <code>sentence-transformers</code>, allowing you to generate vector representations of text locally without relying on external APIs.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ededd122",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_huggingface import HuggingFaceEmbeddings\n",
                "from langchain_chroma import Chroma\n",
                "import numpy as np\n",
                "from sklearn.manifold import TSNE\n",
                "import plotly.graph_objects as go\n",
                "import os\n",
                "\n",
                "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "\n",
                "if os.path.exists(db_name):\n",
                "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
                "\n",
                "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
                "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")\n",
                "\n",
                "collection = vectorstore._collection\n",
                "embedding_result = collection.get(limit=1, include=[\"embeddings\"])\n",
                "embeddings_array = embedding_result.get(\"embeddings\")\n",
                "if embeddings_array is not None and len(embeddings_array) > 0 and embeddings_array[0] is not None:\n",
                "    sample_embedding = embeddings_array[0]\n",
                "    dimensions = len(sample_embedding)\n",
                "    print(f\"The vectors have {dimensions:,} dimensions\")\n",
                "else:\n",
                "    print(\"No embeddings found in the collection.\")\n",
                "\n",
                "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
                "vectors = np.array(result['embeddings'])\n",
                "documents = result['documents']\n",
                "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
                "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]\n",
                "\n",
                "tsne = TSNE(n_components=2, random_state=42)\n",
                "reduced_vectors = tsne.fit_transform(vectors)\n",
                "\n",
                "fig = go.Figure(data=[go.Scatter(\n",
                "    x=reduced_vectors[:, 0],\n",
                "    y=reduced_vectors[:, 1],\n",
                "    mode='markers',\n",
                "    marker=dict(size=5, color=colors, opacity=0.8),\n",
                "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
                "    hoverinfo='text'\n",
                ")])\n",
                "\n",
                "fig.update_layout(\n",
                "    title='2D Chroma Vector Store Visualization',\n",
                "    scene=dict(xaxis_title='x', yaxis_title='y'),\n",
                "    width=800,\n",
                "    height=600,\n",
                "    margin=dict(r=20, b=10, l=10, t=40)\n",
                ")\n",
                "\n",
                "fig.show()\n",
                "\n",
                "tsne = TSNE(n_components=3, random_state=42)\n",
                "reduced_vectors = tsne.fit_transform(vectors)\n",
                "\n",
                "fig = go.Figure(data=[go.Scatter3d(\n",
                "    x=reduced_vectors[:, 0],\n",
                "    y=reduced_vectors[:, 1],\n",
                "    z=reduced_vectors[:, 2],\n",
                "    mode='markers',\n",
                "    marker=dict(size=5, color=colors, opacity=0.8),\n",
                "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
                "    hoverinfo='text'\n",
                ")])\n",
                "\n",
                "fig.update_layout(\n",
                "    title='3D Chroma Vector Store Visualization',\n",
                "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
                "    width=900,\n",
                "    height=700,\n",
                "    margin=dict(r=20, b=10, l=10, t=40)\n",
                ")\n",
                "\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "809c4e6a",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "092c717f",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8de66bfe",
            "metadata": {},
            "source": [
                "#### <code>**day4.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "94ab75ae",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<b style=\"font-size: 16px;\">Abstract:</b> Connect the vector store to a <b>Conversational Retrieval Chain</b>, allowing the LLM to answer questions using retrieved context. Demonstrate how to integrate memory and retrieval for accurate, context-aware responses.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ae835dda",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "import gradio as gr\n",
                "import numpy as np\n",
                "import plotly.graph_objects as go\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
                "from langchain.text_splitter import CharacterTextSplitter\n",
                "from langchain.schema import Document\n",
                "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
                "from langchain_chroma import Chroma\n",
                "from sklearn.manifold import TSNE\n",
                "from langchain.memory import ConversationBufferMemory\n",
                "from langchain.chains import ConversationalRetrievalChain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a7ed3719",
            "metadata": {},
            "outputs": [],
            "source": [
                "# price is a factor for our company, so we're going to use a low cost model\n",
                "MODEL = \"gpt-4o-mini\"\n",
                "db_name = \"vector_db\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e5378ea1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load environment variables in a file called .env\n",
                "load_dotenv(override=True)\n",
                "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "53b486e0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read in documents using LangChain's loaders\n",
                "# Take everything in all the sub-folders of our knowledgebase\n",
                "\n",
                "folders = glob.glob(\"knowledge-base/*\")\n",
                "\n",
                "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
                "text_loader_kwargs = {'encoding': 'utf-8'}\n",
                "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
                "# text_loader_kwargs={'autodetect_encoding': True}\n",
                "\n",
                "documents = []\n",
                "for folder in folders:\n",
                "    doc_type = os.path.basename(folder)\n",
                "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
                "    folder_docs = loader.load()\n",
                "    for doc in folder_docs:\n",
                "        doc.metadata[\"doc_type\"] = doc_type\n",
                "        documents.append(doc)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a54eb304",
            "metadata": {},
            "outputs": [],
            "source": [
                "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "chunks = text_splitter.split_documents(documents)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cfe69534",
            "metadata": {},
            "outputs": [],
            "source": [
                "len(chunks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "58ab1390",
            "metadata": {},
            "outputs": [],
            "source": [
                "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
                "print(f\"Document types found: {', '.join(doc_types)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ff53fad3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
                "# Chroma is a popular open source Vector Database based on SQLLite\n",
                "\n",
                "embeddings = OpenAIEmbeddings()\n",
                "\n",
                "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
                "# Then replace embeddings = OpenAIEmbeddings()\n",
                "# with:\n",
                "# from langchain.embeddings import HuggingFaceEmbeddings\n",
                "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "\n",
                "# Delete if already exists\n",
                "if os.path.exists(db_name):\n",
                "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
                "\n",
                "# Create vectorstore\n",
                "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
                "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1210e57c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get one vector and find how many dimensions it has\n",
                "collection = vectorstore._collection\n",
                "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
                "dimensions = len(sample_embedding)\n",
                "print(f\"The vectors have {dimensions:,} dimensions\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b9d756cd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prework\n",
                "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
                "vectors = np.array(result['embeddings'])\n",
                "documents = result['documents']\n",
                "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
                "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0127d593",
            "metadata": {},
            "outputs": [],
            "source": [
                "# We humans find it easier to visalize things in 2D!\n",
                "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
                "# (t-distributed stochastic neighbor embedding)\n",
                "\n",
                "tsne = TSNE(n_components=2, random_state=42)\n",
                "reduced_vectors = tsne.fit_transform(vectors)\n",
                "\n",
                "# Create the 2D scatter plot\n",
                "fig = go.Figure(data=[go.Scatter(\n",
                "    x=reduced_vectors[:, 0],\n",
                "    y=reduced_vectors[:, 1],\n",
                "    mode='markers',\n",
                "    marker=dict(size=5, color=colors, opacity=0.8),\n",
                "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
                "    hoverinfo='text'\n",
                ")])\n",
                "\n",
                "fig.update_layout(\n",
                "    title='2D Chroma Vector Store Visualization',\n",
                "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
                "    width=800,\n",
                "    height=600,\n",
                "    margin=dict(r=20, b=10, l=10, t=40)\n",
                ")\n",
                "\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6715e57e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's try 3D!\n",
                "\n",
                "tsne = TSNE(n_components=3, random_state=42)\n",
                "reduced_vectors = tsne.fit_transform(vectors)\n",
                "\n",
                "# Create the 3D scatter plot\n",
                "fig = go.Figure(data=[go.Scatter3d(\n",
                "    x=reduced_vectors[:, 0],\n",
                "    y=reduced_vectors[:, 1],\n",
                "    z=reduced_vectors[:, 2],\n",
                "    mode='markers',\n",
                "    marker=dict(size=5, color=colors, opacity=0.8),\n",
                "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
                "    hoverinfo='text'\n",
                ")])\n",
                "\n",
                "fig.update_layout(\n",
                "    title='3D Chroma Vector Store Visualization',\n",
                "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
                "    width=900,\n",
                "    height=700,\n",
                "    margin=dict(r=20, b=10, l=10, t=40)\n",
                ")\n",
                "\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6231f43b",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.3em; font-size: 14px;\">Now it’s time to use <b>LangChain</b> to bring everything together</h5>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c4fd99b0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# create a new Chat with OpenAI\n",
                "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
                "\n",
                "# set up the conversation memory for the chat\n",
                "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
                "\n",
                "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
                "retriever = vectorstore.as_retriever()\n",
                "\n",
                "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
                "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "524227fe",
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"Can you describe Insurellm in a few sentences\"\n",
                "result = conversation_chain.invoke({\"question\":query})\n",
                "print(result[\"answer\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6fc45d46",
            "metadata": {},
            "outputs": [],
            "source": [
                "# set up a new conversation memory for the chat\n",
                "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
                "\n",
                "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
                "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b1e2ea9c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Wrapping in a function - note that history isn't used, as the memory is in the conversation_chain\n",
                "\n",
                "def chat(message, history):\n",
                "    result = conversation_chain.invoke({\"question\": message})\n",
                "    return result[\"answer\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "24ab9ecc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# And in Gradio:\n",
                "\n",
                "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c5683839",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "  <h5 style=\"margin-bottom: 0.2em;\"><b>Questions to Test</b></h5>\n",
                "  <ul style=\"margin: 0.4em 0; padding-left: 1.5em;\">\n",
                "    <li>What awards has Insurellm or its employees received?</li>\n",
                "    <li>Describe the responsibilities of the claims support team.</li>\n",
                "    <li>Which employee is the CEO of Insurellm?</li>\n",
                "    <li>How does Insurellm use AI in its products?</li>\n",
                "    <li>What are the main differences between CarLLM and other insurance products?</li>\n",
                "    <li>Who leads the product development team?</li>\n",
                "    <li>Can you summarize the company’s mission or vision?</li>\n",
                "    <li>What is the process for filing an insurance claim with Insurellm?</li>\n",
                "    <li>Which products are designed for automotive insurance?</li>\n",
                "    <li>Who should I contact for technical support?</li>\n",
                "    <li>What recent innovations has Insurellm introduced?</li>\n",
                "    <li>Are there any notable partnerships or collaborations mentioned in the knowledge base?</li>\n",
                "    <li>What are the eligibility criteria for Insurellm’s insurance products?</li>\n",
                "    <li>How does Insurellm ensure data privacy for its customers?</li>\n",
                "    <li>What is the background of Alex Lancaster?</li>\n",
                "  </ul>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8cd949fa",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "16bc6223",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "15b8c32e",
            "metadata": {},
            "source": [
                "#### <code>**day4.5.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3adfb69e",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<b style=\"font-size: 16px;\">Abstract:</b> Experiment with swapping <b>Chroma</b> for <b>FAISS</b> as the vector database backend. Explore an open-source alternative with the same retrieval workflow.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a1ed3184",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "  This exercise demonstrates how to swap out <b>Chroma</b> for <b>FAISS</b> (Facebook AI Similarity Search) as the vector store backend.<br>\n",
                "  FAISS is an open-source library developed by Facebook AI Research for efficient similarity search on dense vectors.<br>\n",
                "\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d9840999",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "\n",
                "import numpy as np\n",
                "import plotly.graph_objects as go\n",
                "from sklearn.manifold import TSNE\n",
                "from dotenv import load_dotenv\n",
                "import gradio as gr\n",
                "\n",
                "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
                "from langchain.text_splitter import CharacterTextSplitter\n",
                "from langchain.schema import Document\n",
                "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
                "from langchain_chroma import Chroma\n",
                "from langchain.vectorstores import FAISS\n",
                "from langchain.memory import ConversationBufferMemory\n",
                "from langchain.chains import ConversationalRetrievalChain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71cffeb7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# price is a factor for our company, so we're going to use a low cost model\n",
                "\n",
                "MODEL = \"gpt-4o-mini\"\n",
                "db_name = \"vector_db\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1f6c8c41",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load environment variables in a file called .env\n",
                "\n",
                "load_dotenv(override=True)\n",
                "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fe8fd1c1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read in documents using LangChain's loaders\n",
                "# Take everything in all the sub-folders of our knowledgebase\n",
                "\n",
                "folders = glob.glob(\"knowledge-base/*\")\n",
                "\n",
                "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
                "text_loader_kwargs = {'encoding': 'utf-8'}\n",
                "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
                "# text_loader_kwargs={'autodetect_encoding': True}\n",
                "\n",
                "documents = []\n",
                "for folder in folders:\n",
                "    doc_type = os.path.basename(folder)\n",
                "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
                "    folder_docs = loader.load()\n",
                "    for doc in folder_docs:\n",
                "        doc.metadata[\"doc_type\"] = doc_type\n",
                "        documents.append(doc)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aced1b20",
            "metadata": {},
            "outputs": [],
            "source": [
                "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "chunks = text_splitter.split_documents(documents)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5865b95a",
            "metadata": {},
            "outputs": [],
            "source": [
                "len(chunks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cd672df4",
            "metadata": {},
            "outputs": [],
            "source": [
                "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
                "print(f\"Document types found: {', '.join(doc_types)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ebcce82f",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "  <h5 style=\"margin-bottom: 0.2em; font-size: 15px;\"><b>A Sidenote on Embeddings and Auto-Encoding LLMs</b></h5>\n",
                "  We will be mapping each chunk of text into a vector that captures its semantic meaning — this is called an <b>embedding</b>.<br>\n",
                "\n",
                "  OpenAI provides an embedding model that we'll use through their API, integrated with LangChain code.<br>\n",
                "\n",
                "  This model is an example of an <b>Auto-Encoding LLM</b>, which processes the entire input to produce a single embedding vector. This differs from <b>Auto-Regressive LLMs</b> (like GPT), which generate output token by token based on prior context.<br>\n",
                "\n",
                "  A well-known auto-encoding model is <b>BERT</b> by Google. In addition to producing embeddings, such models are widely used for tasks like classification and entity recognition.<br>\n",
                "\n",
                "  <!-- <b>Sidenote:</b> In <b>Week 8</b>, we’ll revisit RAG and embeddings, using an open-source vector encoder locally so that no data is sent to external APIs — a key requirement in many enterprise deployments. -->\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8627e9d2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
                "# Chroma is a popular open source Vector Database based on SQLLite\n",
                "\n",
                "embeddings = OpenAIEmbeddings()\n",
                "\n",
                "# Create vectorstore\n",
                "\n",
                "# BEFORE\n",
                "# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
                "\n",
                "# AFTER\n",
                "vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
                "\n",
                "total_vectors = vectorstore.index.ntotal\n",
                "dimensions = vectorstore.index.d\n",
                "\n",
                "print(f\"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aafd86ff",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prework\n",
                "vectors = []\n",
                "documents = []\n",
                "doc_types = []\n",
                "colors = []\n",
                "color_map = {'products':'blue', 'employees':'green', 'contracts':'red', 'company':'orange'}\n",
                "\n",
                "for i in range(total_vectors):\n",
                "    vectors.append(vectorstore.index.reconstruct(i))\n",
                "    doc_id = vectorstore.index_to_docstore_id[i]\n",
                "    document = vectorstore.docstore.search(doc_id)\n",
                "    documents.append(document.page_content)\n",
                "    doc_type = document.metadata['doc_type']\n",
                "    doc_types.append(doc_type)\n",
                "    colors.append(color_map[doc_type])\n",
                "    \n",
                "vectors = np.array(vectors)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "893704ef",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "  <h5 style=\"margin-bottom: 0.2em;\"><b>Visualizing the Vector Store</b></h5>\n",
                "  Let's take a moment to examine the documents and their corresponding embedding vectors to better understand how the system organizes and retrieves information.<br>\n",
                "\n",
                "  <i>Sidenote:</i> What we’re really visualizing here is the <b>distribution of vector embeddings</b> generated by <code>OpenAIEmbeddings</code> and retrieved from the <code>FAISS</code> index. Naturally, these visualizations will appear identical whether the vectors are stored in <b>FAISS</b> or <b>Chroma</b>—since the underlying embeddings remain the same.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "faf6582a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# We humans find it easier to visalize things in 2D!\n",
                "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
                "# (t-distributed stochastic neighbor embedding)\n",
                "\n",
                "tsne = TSNE(n_components=2, random_state=42)\n",
                "reduced_vectors = tsne.fit_transform(vectors)\n",
                "\n",
                "# Create the 2D scatter plot\n",
                "fig = go.Figure(data=[go.Scatter(\n",
                "    x=reduced_vectors[:, 0],\n",
                "    y=reduced_vectors[:, 1],\n",
                "    mode='markers',\n",
                "    marker=dict(size=5, color=colors, opacity=0.8),\n",
                "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
                "    hoverinfo='text'\n",
                ")])\n",
                "\n",
                "fig.update_layout(\n",
                "    title='2D FAISS Vector Store Visualization',\n",
                "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
                "    width=800,\n",
                "    height=600,\n",
                "    margin=dict(r=20, b=10, l=10, t=40)\n",
                ")\n",
                "\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "14bd5aee",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's try 3D!\n",
                "\n",
                "tsne = TSNE(n_components=3, random_state=42)\n",
                "reduced_vectors = tsne.fit_transform(vectors)\n",
                "\n",
                "# Create the 3D scatter plot\n",
                "fig = go.Figure(data=[go.Scatter3d(\n",
                "    x=reduced_vectors[:, 0],\n",
                "    y=reduced_vectors[:, 1],\n",
                "    z=reduced_vectors[:, 2],\n",
                "    mode='markers',\n",
                "    marker=dict(size=5, color=colors, opacity=0.8),\n",
                "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
                "    hoverinfo='text'\n",
                ")])\n",
                "\n",
                "fig.update_layout(\n",
                "    title='3D FAISS Vector Store Visualization',\n",
                "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
                "    width=900,\n",
                "    height=700,\n",
                "    margin=dict(r=20, b=10, l=10, t=40)\n",
                ")\n",
                "\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d76c02ad",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.3em; font-size: 14px;\">Now it’s time to use <b>LangChain</b> to bring everything together</h5>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8f393e1c",
            "metadata": {},
            "outputs": [],
            "source": [
                "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
                "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
                "\n",
                "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
                "retriever = vectorstore.as_retriever()\n",
                "\n",
                "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
                "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "384918cc",
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"Can you describe Insurellm in a few sentences\"\n",
                "result = conversation_chain.invoke({\"question\":query})\n",
                "print(result[\"answer\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6d7d7309",
            "metadata": {},
            "outputs": [],
            "source": [
                "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
                "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2b0ed77c",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "  <h5 style=\"margin-bottom: 0.2em;\"><b>Bringing It to Life with Gradio</b></h5>\n",
                "  Now we will bring this up in <b>Gradio</b> using the <code>ChatInterface</code> — a quick and easy way to prototype a conversational interface with an LLM.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2c6a6789",
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat(message, history):\n",
                "    result = conversation_chain.invoke({\"question\": message})\n",
                "    return result[\"answer\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8b6cf9bc",
            "metadata": {},
            "outputs": [],
            "source": [
                "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4675edf5",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5;\">\n",
                "  <h4 style=\"margin-bottom: 0.5em;\"><b>Comparison: Chroma vs FAISS (Facebook AI Similarity Search)</b></h4>\n",
                "  <table style=\"border-collapse: collapse; width: 100%; font-size: 14px;\">\n",
                "    <thead>\n",
                "      <tr>\n",
                "        <th style=\"border: 1px solid #ccc; padding: 8px;\">Criteria</th>\n",
                "        <th style=\"border: 1px solid #ccc; padding: 8px;\">Chroma</th>\n",
                "        <th style=\"border: 1px solid #ccc; padding: 8px;\">FAISS</th>\n",
                "      </tr>\n",
                "    </thead>\n",
                "    <tbody>\n",
                "      <tr>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Origin</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Open-source, developed by the ChromaDB team</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Open-source, developed by Facebook AI Research (FAIR)</td>\n",
                "      </tr>\n",
                "      <tr>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Supported OS</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Cross-platform (Windows, Linux, macOS)</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Linux, macOS (GPU supported); Windows supports only CPU (GPU setup is difficult)</td>\n",
                "      </tr>\n",
                "      <tr>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Installation</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\"><code>pip install chromadb</code> or via LangChain</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\"><code>pip install faiss-cpu</code> (for CPU) or <code>faiss-gpu</code> (GPU, Linux/macOS only)</td>\n",
                "      </tr>\n",
                "      <tr>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">LangChain Integration</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Yes, via <code>langchain_chroma.Chroma</code></td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Yes, via <code>langchain.vectorstores.FAISS</code></td>\n",
                "      </tr>\n",
                "      <tr>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Data Storage</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Supports disk persistence, multiple collections</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">In-memory; can save/load FAISS binary files</td>\n",
                "      </tr>\n",
                "      <tr>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Scalability</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Good for small to medium apps, can be used in production</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Very strong for large-scale data, optimized for high-performance search</td>\n",
                "      </tr>\n",
                "      <tr>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">GPU Support</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">❌ Not supported</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">✅ Supported (Linux/macOS with CUDA + faiss-gpu)</td>\n",
                "      </tr>\n",
                "      <tr>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Search API</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Easy to use, supports filters, metadata, various queries</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Optimized for vector search (nearest neighbor), lacks advanced filter/metadata support</td>\n",
                "      </tr>\n",
                "      <tr>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Key Features</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">- Simple to use<br>- Good metadata management<br>- Filter support</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">- Extremely fast vector search<br>- Supports many ANN algorithms<br>- GPU support available</td>\n",
                "      </tr>\n",
                "      <tr>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Compatibility</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Good with Windows, ideal for learning/lab</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Not recommended for GPU use on Windows; best on Linux for large-scale production</td>\n",
                "      </tr>\n",
                "      <tr>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Use Cases</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Prototyping, demos, small to medium apps, filter-required cases</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Large-scale production, millions of vectors, high-speed requirements</td>\n",
                "      </tr>\n",
                "      <tr>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Drawbacks</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Not optimized for huge datasets, lacks GPU support</td>\n",
                "        <td style=\"border: 1px solid #ccc; padding: 8px;\">Complex GPU setup on Windows, weak metadata/filter handling</td>\n",
                "      </tr>\n",
                "    </tbody>\n",
                "  </table>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b16a91b8",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b98568ad",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "11d1def1",
            "metadata": {},
            "source": [
                "#### <code>**day5.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0d26c5a8",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<b style=\"font-size: 16px;\">Abstract:</b> Build a complete <b>Gradio chat interface</b> to interact with the given knowledge base. This final integration delivers a low-cost, fully interactive Q&A assistant suitable for enterprise use.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2abe7c1a",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<b>Note:</b> The following model uses the <code>test-base</code> knowledge base.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "300f6835",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "import gradio as gr\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from sklearn.manifold import TSNE\n",
                "import plotly.graph_objects as go\n",
                "\n",
                "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
                "from langchain.text_splitter import CharacterTextSplitter\n",
                "from langchain.schema import Document\n",
                "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
                "from langchain_chroma import Chroma\n",
                "from langchain.memory import ConversationBufferMemory\n",
                "from langchain.chains import ConversationalRetrievalChain\n",
                "from langchain.embeddings import HuggingFaceEmbeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d127f69",
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL = \"gpt-4o-mini\"\n",
                "db_name = \"vector_db\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aa5d7a64",
            "metadata": {},
            "outputs": [],
            "source": [
                "load_dotenv(override=True)\n",
                "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "99f5c7b6",
            "metadata": {},
            "outputs": [],
            "source": [
                "system_message = (\n",
                "    \"You are an expert in answering accurate questions about the knowledge base. \"\n",
                "    \"Always answer in English. If you don't know the answer, say so. \"\n",
                "    \"Do not make anything up if you haven't been provided with relevant context.\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "62607013",
            "metadata": {},
            "outputs": [],
            "source": [
                "folders = glob.glob(\"test-base/*\")\n",
                "\n",
                "def add_metadata(doc, doc_type):\n",
                "    doc.metadata[\"doc_type\"] = doc_type\n",
                "    return doc\n",
                "\n",
                "text_loader_kwargs = {'encoding': 'utf-8'}\n",
                "\n",
                "documents = []\n",
                "for folder in folders:\n",
                "    doc_type = os.path.basename(folder)\n",
                "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
                "    folder_docs = loader.load()\n",
                "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
                "\n",
                "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "chunks = text_splitter.split_documents(documents)\n",
                "\n",
                "print(f\"Total number of chunks: {len(chunks)}\")\n",
                "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "53f5f266",
            "metadata": {},
            "outputs": [],
            "source": [
                "embeddings = OpenAIEmbeddings()\n",
                "\n",
                "if os.path.exists(db_name):\n",
                "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
                "\n",
                "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
                "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ccb55db5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's investigate the vectors\n",
                "\n",
                "collection = vectorstore._collection\n",
                "count = collection.count()\n",
                "\n",
                "embedding_result = collection.get(limit=1, include=[\"embeddings\"])\n",
                "embeddings_array = embedding_result.get(\"embeddings\")\n",
                "if embeddings_array is not None and len(embeddings_array) > 0 and embeddings_array[0] is not None:\n",
                "\tsample_embedding = embeddings_array[0]\n",
                "\tdimensions = len(sample_embedding)\n",
                "\tprint(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")\n",
                "else:\n",
                "\tprint(\"No embeddings found in the collection.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6cfd63b9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prework (with thanks to Jon R for identifying and fixing a bug in this!)\n",
                "\n",
                "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
                "vectors = np.array(result['embeddings'])\n",
                "documents = result['documents']\n",
                "metadatas = result['metadatas']\n",
                "doc_types = [metadata['doc_type'] for metadata in metadatas if metadata is not None]\n",
                "color_map = {\n",
                "\t'algorithms': 'blue',\n",
                "\t'applications': 'green',\n",
                "\t'datasets': 'red',\n",
                "\t'researchers': 'orange'\n",
                "}\n",
                "colors = [color_map.get(t, 'gray') for t in doc_types if t is not None]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "674ac231",
            "metadata": {},
            "outputs": [],
            "source": [
                "tsne = TSNE(n_components=2, random_state=42, perplexity=2)\n",
                "reduced_vectors = tsne.fit_transform(vectors)\n",
                "fig = go.Figure(data=[go.Scatter(\n",
                "    x=reduced_vectors[:, 0],\n",
                "    y=reduced_vectors[:, 1],\n",
                "    mode='markers',\n",
                "    marker=dict(size=5, color=colors, opacity=0.8),\n",
                "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
                "    hoverinfo='text'\n",
                ")])\n",
                "fig.update_layout(\n",
                "    title='2D Chroma Vector Store Visualization',\n",
                "    scene=dict(xaxis_title='x', yaxis_title='y'),\n",
                "    width=800,\n",
                "    height=600,\n",
                "    margin=dict(r=20, b=10, l=10, t=40)\n",
                ")\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d66c2c93",
            "metadata": {},
            "outputs": [],
            "source": [
                "tsne = TSNE(n_components=3, random_state=42, perplexity=2)\n",
                "reduced_vectors = tsne.fit_transform(vectors)\n",
                "fig = go.Figure(data=[go.Scatter3d(\n",
                "    x=reduced_vectors[:, 0],\n",
                "    y=reduced_vectors[:, 1],\n",
                "    z=reduced_vectors[:, 2],\n",
                "    mode='markers',\n",
                "    marker=dict(size=5, color=colors, opacity=0.8),\n",
                "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
                "    hoverinfo='text'\n",
                ")])\n",
                "fig.update_layout(\n",
                "    title='3D Chroma Vector Store Visualization',\n",
                "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
                "    width=900,\n",
                "    height=700,\n",
                "    margin=dict(r=20, b=10, l=10, t=40)\n",
                ")\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "19a1f0eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
                "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
                "retriever = vectorstore.as_retriever()\n",
                "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4b25696a",
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"Who is Geoffrey Hinton?\"\n",
                "result = conversation_chain.invoke({\"question\": query})\n",
                "print(result[\"answer\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6b4df8e9",
            "metadata": {},
            "outputs": [],
            "source": [
                "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
                "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7fed7885",
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat(question, history):\n",
                "    result = conversation_chain.invoke({\"question\": question})\n",
                "    return result[\"answer\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b2188ad8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# And in Gradio:\n",
                "\n",
                "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bb8e7653",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's investigate what gets sent behind the scenes\n",
                "\n",
                "from langchain_core.callbacks import StdOutCallbackHandler\n",
                "\n",
                "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
                "\n",
                "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
                "\n",
                "retriever = vectorstore.as_retriever()\n",
                "\n",
                "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])\n",
                "\n",
                "query = \"Who is Geoffrey Hinton?\"\n",
                "result = conversation_chain.invoke({\"question\": query})\n",
                "answer = result[\"answer\"]\n",
                "print(\"\\nAnswer:\", answer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6ecb1782",
            "metadata": {},
            "outputs": [],
            "source": [
                "# create a new Chat with OpenAI\n",
                "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
                "\n",
                "# set up the conversation memory for the chat\n",
                "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
                "\n",
                "# the retriever is an abstraction over the VectorStore that will be used during RAG; k is how many chunks to use\n",
                "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
                "\n",
                "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
                "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1b8e8865",
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat(question, history):\n",
                "    result = conversation_chain.invoke({\"question\": question})\n",
                "    return result[\"answer\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ec9214a4",
            "metadata": {},
            "outputs": [],
            "source": [
                "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "llms-gpu",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
