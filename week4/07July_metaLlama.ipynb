{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a53b04",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 13px; line-height: 1.4; margin: 0; padding: 0;\">\n",
    "<h5 style=\"margin-bottom: 0.2em; font-size: 14px;\">\n",
    "<b>Note:</b> This notebook contains testing and evaluation of the <code>codellama:7b-python</code> model conducted on <b>July 7, 2025</b>, with the goal of refining the system prompt for <code>Llama3.2</code> in <code>07July.ipynb</code>.\n",
    "</h5>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9faff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages if not already installed\n",
    "# %pip install torch transformers accelerate --quiet\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.pipelines import pipeline\n",
    "import subprocess\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\")\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab59ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_message_codellama = (\n",
    "#     \"ALWAYS output a complete C++17 program with a main() function that prints the result to standard output. \"\n",
    "#     \"Your output MUST be a single, self-contained C++ file that compiles and runs as-is. \"\n",
    "#     \"Do not omit the main() function. \"\n",
    "#     \"You are a high-performance Python-to-C++ reimplementation assistant for Windows. \"\n",
    "#     \"Always include all necessary headers: #include <iostream>, #include <chrono>, #include <cstdint>, #include <limits>, #include <iomanip>, #include <vector>, #include <cmath>. \"\n",
    "#     \"Avoid using or inventing types that do not exist in standard C++ (such as uint128_t, uint256_t). Only use uint64_t, int64_t, double, etc. \"\n",
    "#     \"Do not include numeric literals with underscores (such as 10_000_000), but rather only use standard C++ number literals (such as 10000000). \"\n",
    "#     \"Avoid using template syntax like vector<int64_t>::size in numeric_limits or anywhere else. \"\n",
    "#     \"Do not define or overload operators for built-in types, such as operator% for uint64_t. \"\n",
    "#     \"Avoid custom namespaces and templates unless absolutely required by C++17. \"\n",
    "#     \"Avoid Python-specific keywords (such as 'yield'). Avoid invalid casts or pointer casts in arithmetic; only use static_cast for valid type conversions. \"\n",
    "#     \"Avoid using user-defined literals, literal suffixes that are not standard C++, such as 100_000_000 or 10ms).\"\n",
    "#     \"Avoid old-style base class initializers and unnamed initializers in constructors unless absolutely required by C++17.\"\n",
    "#     \"All variables, constants, functions must be declared before their first use. This includes any constants (e.g., J, K) which should be declared as const variables within an appropriate scope such as global, namespace or function scope.\"\n",
    "#     \"Avoid using custom types that are not explicitly defined in the code and avoid invalid identifiers outside of C++17 standards.\"\n",
    "#     \"Use double quotes for string literals instead of single quotes. Always use std:: for standard library functions and manipulators (e.g., std::fixed, std::setprecision).\"\n",
    "#     \"For random number generation: only allow custom linear congruential generators if the Python code uses it; never include <random> or std::mt19937_64 unless required by python.\"\n",
    "#     \"Use simple classes with next() methods for generators instead of complex iterators. Use int64_t consistently when dealing with 2^32 and use uint64_t only if necessary, such as in min_val = -10.\"\n",
    "#     \"Avoid incrementing const references or using undefined functions or variables; match variable names exactly from function parameters to prevent confusion between different scopes of the code. \"\n",
    "#     \"Prefer simple structures over complex templates when possible – try not to use them unless absolutely required by C++17 standards.\" \n",
    "#     \"Ensure that algorithms are identical in both Python and generated C++ versions, written with simplicity for readability.\"\n",
    "#     \"Only include standard library functions; no custom undefined ones should be used. All code must compile successfully using g++ -std=c++17 without requiring any features or syntax beyond c++17 standards.\" \n",
    "#     \"For custom numeric types (e.g., Int64), provide non-explicit constructors for implicit conversions from standard integer types like int64_t and uint64_t, such as Int64(uint64_t val).\"\n",
    "#     \"If a custom class has private data members that need external access, offer public getter methods (e.g., long long getValue() const;) or public conversion operators (e.g., operator int64_t() const;)\"\n",
    "#     \"For custom numeric types: explicitly overload all necessary arithmetic and comparison operators (e.g., +, -, *, /, %, ==, !=, <, <=, >, >=) to support interactions with both other custom types as well as built-in ones like int64_t or uint64_t. Ensure symmetric operations are supported too – e.g., CustomType op BuiltInType and BuiltInType op CustomType.\"\n",
    "#     \"For floating-point modulo operations: always use std::fmod from the <cmath> header instead of % operator which is strictly for integer types only.\" \n",
    "#     \"Provide valid C++ code that compiles without errors or warnings. \"\n",
    "#     \"Output a complete C++17 program with a main() function that prints the result.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a097e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(python):\n",
    "    user_prompt = (\n",
    "        \"IMPORTANT: Output a complete, compilable C++17 program with a main() function that prints the result. \"\n",
    "        \"Do not omit the main() function. \"\n",
    "        \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
    "        \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
    "        \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
    "    )\n",
    "    user_prompt += python\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7642256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_llama_hf(python_code, max_new_tokens=2048):\n",
    "    system_prompt = (\n",
    "        \"IMPORTANT: Output a complete, compilable C++17 program with a main() function that prints the result. \"\n",
    "        \"Do not omit the main() function. \"\n",
    "        \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
    "        \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
    "        \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
    "    )\n",
    "    prompt = system_prompt + python_code\n",
    "    # Generate C++ code\n",
    "    output = llama_pipe(prompt, max_new_tokens=max_new_tokens, do_sample=True)\n",
    "    # Yield the generated code (simulate streaming)\n",
    "    for line in output[0]['generated_text'].splitlines():\n",
    "        yield line + \"\\n\"\n",
    "\n",
    "def write_output(cpp, filename=\"optimized_codellama.cpp\"):\n",
    "    code = cpp.replace(\"```cpp\",\"\").replace(\"```\",\"\")\n",
    "    lines = code.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if (not stripped.startswith('Note that') and \n",
    "            not stripped.startswith('The ') and\n",
    "            not stripped.startswith('This ') and\n",
    "            '`' not in stripped): \n",
    "            cleaned_lines.append(line)\n",
    "    cleaned_code = '\\n'.join(cleaned_lines)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(cleaned_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0abca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_cpp(code):\n",
    "    write_output(code, \"optimized_codellama.cpp\")\n",
    "    try:\n",
    "        compile_cmd = [\"g++\", \"-O3\", \"-std=c++17\", \"-o\", \"optimized_codellama.exe\", \"optimized_codellama.cpp\"]\n",
    "        compile_result = subprocess.run(compile_cmd, check=True, text=True, capture_output=True)\n",
    "        run_cmd = [\"optimized_codellama.exe\"]\n",
    "        run_result = subprocess.run(run_cmd, check=True, text=True, capture_output=True)\n",
    "        return run_result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"An error occurred:\\n{e.stderr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\"\n",
    "\n",
    "python_hard = \"\"\"# Be careful to support large number sizes\n",
    "\n",
    "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
    "    value = seed\n",
    "    while True:\n",
    "        value = (a * value + c) % m\n",
    "        yield value\n",
    "        \n",
    "def max_subarray_sum(n, seed, min_val, max_val):\n",
    "    lcg_gen = lcg(seed)\n",
    "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
    "    max_sum = float('-inf')\n",
    "    for i in range(n):\n",
    "        current_sum = 0\n",
    "        for j in range(i, n):\n",
    "            current_sum += random_numbers[j]\n",
    "            if current_sum > max_sum:\n",
    "                max_sum = current_sum\n",
    "    return max_sum\n",
    "\n",
    "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
    "    total_sum = 0\n",
    "    lcg_gen = lcg(initial_seed)\n",
    "    for _ in range(20):\n",
    "        seed = next(lcg_gen)\n",
    "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
    "    return total_sum\n",
    "\n",
    "# Parameters\n",
    "n = 10000         # Number of random numbers\n",
    "initial_seed = 42 # Initial seed for the LCG\n",
    "min_val = -10     # Minimum value of random numbers\n",
    "max_val = 10      # Maximum value of random numbers\n",
    "\n",
    "# Timing the function\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
    "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llama_hf_on_problem(python_code, output_cpp=\"optimized_llama31.cpp\"):\n",
    "    print(\"=== Generating with Llama-3.1-8B-Instruct ===\")\n",
    "    cpp_code = \"\"\n",
    "    for chunk in stream_llama_hf(python_code):\n",
    "        print(chunk, end='', flush=True)\n",
    "        cpp_code += chunk\n",
    "    print(\"\\n=== Writing and compiling ===\")\n",
    "    write_output(cpp_code, output_cpp)\n",
    "    compile_cmd = [\"g++\", \"-O3\", \"-std=c++17\", \"-o\", \"optimized_llama31.exe\", output_cpp]\n",
    "    compile_result = subprocess.run(compile_cmd, capture_output=True, text=True)\n",
    "    if compile_result.returncode != 0:\n",
    "        print(f\"Compilation error: {compile_result.stderr}\")\n",
    "        return\n",
    "    print(\"=== Running executable ===\")\n",
    "    run_cmd = [\"optimized_llama31.exe\"]\n",
    "    run_result = subprocess.run(run_cmd, capture_output=True, text=True)\n",
    "    print(run_result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d225ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Replace your test cells with these ---\n",
    "print(\"Testing Pi Calculation with Llama-3.1-8B-Instruct\")\n",
    "run_llama_hf_on_problem(pi)\n",
    "\n",
    "print(\"Testing Maximum Subarray Sum with Llama-3.1-8B-Instruct\")\n",
    "run_llama_hf_on_problem(python_hard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
