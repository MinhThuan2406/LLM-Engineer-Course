{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a53b04",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 13px; line-height: 1.4; margin: 0; padding: 0;\">\n",
    "<h5 style=\"margin-bottom: 0.2em; font-size: 14px;\">\n",
    "<b>Note:</b> This notebook contains testing and evaluation of the <code>codellama:7b-python</code> model conducted on <b>July 7, 2025</b>, with the goal of refining the system prompt for <code>Llama3.2</code> in <code>07July.ipynb</code>.\n",
    "</h5>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc9faff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found. Error loading \"c:\\Users\\PC\\anaconda3\\envs\\llms\\Lib\\site-packages\\torch\\lib\\c10_cuda.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Install required packages if not already installed\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# !pip install transformers accelerate\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# If you see CUDA DLL errors, install CPU-only torch and set device_map to \"cpu\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# %pip install torch --index-url https://download.pytorch.org/whl/cpu\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipelines\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     logging,\n\u001b[32m     50\u001b[39m )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m define_import_structure\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\utils\\__init__.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01margs_doc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     ClassAttrs,\n\u001b[32m     26\u001b[39m     ClassDocstring,\n\u001b[32m     27\u001b[39m     ImageProcessorArgs,\n\u001b[32m     28\u001b[39m     ModelArgs,\n\u001b[32m     29\u001b[39m     ModelOutputArgs,\n\u001b[32m     30\u001b[39m     auto_class_docstring,\n\u001b[32m     31\u001b[39m     auto_docstring,\n\u001b[32m     32\u001b[39m     get_args_doc_from_source,\n\u001b[32m     33\u001b[39m     parse_docstring,\n\u001b[32m     34\u001b[39m     set_min_indent,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\utils\\args_doc.py:30\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mregex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     MODELS_TO_PIPELINE,\n\u001b[32m     26\u001b[39m     PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS,\n\u001b[32m     27\u001b[39m     PT_SAMPLE_DOCSTRINGS,\n\u001b[32m     28\u001b[39m     _prepare_output_docstrings,\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[32m     33\u001b[39m PATH_TO_TRANSFORMERS = Path(\u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m).resolve() / \u001b[33m\"\u001b[39m\u001b[33mtransformers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m AUTODOC_FILES = [\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfiguration_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodeling_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature_extractor_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\utils\\generic.py:46\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     35\u001b[39m     get_torch_version,\n\u001b[32m     36\u001b[39m     is_flax_available,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     is_torch_fx_proxy,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# required for @can_return_tuple decorator to work with torchdynamo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcached_property\u001b[39;00m(\u001b[38;5;28mproperty\u001b[39m):\n\u001b[32m     50\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    Descriptor that mimics @property but caches output in member variable.\u001b[39;00m\n\u001b[32m     52\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m \u001b[33;03m    Built-in in functools from Python 3.8.\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\llms\\Lib\\site-packages\\torch\\__init__.py:137\u001b[39m\n\u001b[32m    135\u001b[39m     err = ctypes.WinError(last_error)\n\u001b[32m    136\u001b[39m     err.strerror += \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Error loading \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m or one of its dependencies.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    139\u001b[39m     is_loaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [WinError 127] The specified procedure could not be found. Error loading \"c:\\Users\\PC\\anaconda3\\envs\\llms\\Lib\\site-packages\\torch\\lib\\c10_cuda.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install transformers accelerate\n",
    "\n",
    "# If you see CUDA DLL errors, install CPU-only torch and set device_map to \"cpu\"\n",
    "# %pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.pipelines import pipeline\n",
    "import subprocess\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\")\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab59ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_message_codellama = (\n",
    "#     \"ALWAYS output a complete C++17 program with a main() function that prints the result to standard output. \"\n",
    "#     \"Your output MUST be a single, self-contained C++ file that compiles and runs as-is. \"\n",
    "#     \"Do not omit the main() function. \"\n",
    "#     \"You are a high-performance Python-to-C++ reimplementation assistant for Windows. \"\n",
    "#     \"Always include all necessary headers: #include <iostream>, #include <chrono>, #include <cstdint>, #include <limits>, #include <iomanip>, #include <vector>, #include <cmath>. \"\n",
    "#     \"Avoid using or inventing types that do not exist in standard C++ (such as uint128_t, uint256_t). Only use uint64_t, int64_t, double, etc. \"\n",
    "#     \"Do not include numeric literals with underscores (such as 10_000_000), but rather only use standard C++ number literals (such as 10000000). \"\n",
    "#     \"Avoid using template syntax like vector<int64_t>::size in numeric_limits or anywhere else. \"\n",
    "#     \"Do not define or overload operators for built-in types, such as operator% for uint64_t. \"\n",
    "#     \"Avoid custom namespaces and templates unless absolutely required by C++17. \"\n",
    "#     \"Avoid Python-specific keywords (such as 'yield'). Avoid invalid casts or pointer casts in arithmetic; only use static_cast for valid type conversions. \"\n",
    "#     \"Avoid using user-defined literals, literal suffixes that are not standard C++, such as 100_000_000 or 10ms).\"\n",
    "#     \"Avoid old-style base class initializers and unnamed initializers in constructors unless absolutely required by C++17.\"\n",
    "#     \"All variables, constants, functions must be declared before their first use. This includes any constants (e.g., J, K) which should be declared as const variables within an appropriate scope such as global, namespace or function scope.\"\n",
    "#     \"Avoid using custom types that are not explicitly defined in the code and avoid invalid identifiers outside of C++17 standards.\"\n",
    "#     \"Use double quotes for string literals instead of single quotes. Always use std:: for standard library functions and manipulators (e.g., std::fixed, std::setprecision).\"\n",
    "#     \"For random number generation: only allow custom linear congruential generators if the Python code uses it; never include <random> or std::mt19937_64 unless required by python.\"\n",
    "#     \"Use simple classes with next() methods for generators instead of complex iterators. Use int64_t consistently when dealing with 2^32 and use uint64_t only if necessary, such as in min_val = -10.\"\n",
    "#     \"Avoid incrementing const references or using undefined functions or variables; match variable names exactly from function parameters to prevent confusion between different scopes of the code. \"\n",
    "#     \"Prefer simple structures over complex templates when possible – try not to use them unless absolutely required by C++17 standards.\" \n",
    "#     \"Ensure that algorithms are identical in both Python and generated C++ versions, written with simplicity for readability.\"\n",
    "#     \"Only include standard library functions; no custom undefined ones should be used. All code must compile successfully using g++ -std=c++17 without requiring any features or syntax beyond c++17 standards.\" \n",
    "#     \"For custom numeric types (e.g., Int64), provide non-explicit constructors for implicit conversions from standard integer types like int64_t and uint64_t, such as Int64(uint64_t val).\"\n",
    "#     \"If a custom class has private data members that need external access, offer public getter methods (e.g., long long getValue() const;) or public conversion operators (e.g., operator int64_t() const;)\"\n",
    "#     \"For custom numeric types: explicitly overload all necessary arithmetic and comparison operators (e.g., +, -, *, /, %, ==, !=, <, <=, >, >=) to support interactions with both other custom types as well as built-in ones like int64_t or uint64_t. Ensure symmetric operations are supported too – e.g., CustomType op BuiltInType and BuiltInType op CustomType.\"\n",
    "#     \"For floating-point modulo operations: always use std::fmod from the <cmath> header instead of % operator which is strictly for integer types only.\" \n",
    "#     \"Provide valid C++ code that compiles without errors or warnings. \"\n",
    "#     \"Output a complete C++17 program with a main() function that prints the result.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a097e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(python):\n",
    "    user_prompt = (\n",
    "        \"IMPORTANT: Output a complete, compilable C++17 program with a main() function that prints the result. \"\n",
    "        \"Do not omit the main() function. \"\n",
    "        \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
    "        \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
    "        \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
    "    )\n",
    "    user_prompt += python\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7642256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_llama_hf(python_code, max_new_tokens=2048):\n",
    "    system_prompt = (\n",
    "        \"IMPORTANT: Output a complete, compilable C++17 program with a main() function that prints the result. \"\n",
    "        \"Do not omit the main() function. \"\n",
    "        \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
    "        \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
    "        \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
    "    )\n",
    "    prompt = system_prompt + python_code\n",
    "    # Generate C++ code\n",
    "    output = llama_pipe(prompt, max_new_tokens=max_new_tokens, do_sample=True)\n",
    "    # Yield the generated code (simulate streaming)\n",
    "    for line in output[0]['generated_text'].splitlines():\n",
    "        yield line + \"\\n\"\n",
    "\n",
    "def write_output(cpp, filename=\"optimized_codellama.cpp\"):\n",
    "    code = cpp.replace(\"```cpp\",\"\").replace(\"```\",\"\")\n",
    "    lines = code.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if (not stripped.startswith('Note that') and \n",
    "            not stripped.startswith('The ') and\n",
    "            not stripped.startswith('This ') and\n",
    "            '`' not in stripped): \n",
    "            cleaned_lines.append(line)\n",
    "    cleaned_code = '\\n'.join(cleaned_lines)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(cleaned_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0abca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_cpp(code):\n",
    "    write_output(code, \"optimized_codellama.cpp\")\n",
    "    try:\n",
    "        compile_cmd = [\"g++\", \"-O3\", \"-std=c++17\", \"-o\", \"optimized_codellama.exe\", \"optimized_codellama.cpp\"]\n",
    "        compile_result = subprocess.run(compile_cmd, check=True, text=True, capture_output=True)\n",
    "        run_cmd = [\"optimized_codellama.exe\"]\n",
    "        run_result = subprocess.run(run_cmd, check=True, text=True, capture_output=True)\n",
    "        return run_result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"An error occurred:\\n{e.stderr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\"\n",
    "\n",
    "python_hard = \"\"\"# Be careful to support large number sizes\n",
    "\n",
    "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
    "    value = seed\n",
    "    while True:\n",
    "        value = (a * value + c) % m\n",
    "        yield value\n",
    "        \n",
    "def max_subarray_sum(n, seed, min_val, max_val):\n",
    "    lcg_gen = lcg(seed)\n",
    "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
    "    max_sum = float('-inf')\n",
    "    for i in range(n):\n",
    "        current_sum = 0\n",
    "        for j in range(i, n):\n",
    "            current_sum += random_numbers[j]\n",
    "            if current_sum > max_sum:\n",
    "                max_sum = current_sum\n",
    "    return max_sum\n",
    "\n",
    "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
    "    total_sum = 0\n",
    "    lcg_gen = lcg(initial_seed)\n",
    "    for _ in range(20):\n",
    "        seed = next(lcg_gen)\n",
    "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
    "    return total_sum\n",
    "\n",
    "# Parameters\n",
    "n = 10000         # Number of random numbers\n",
    "initial_seed = 42 # Initial seed for the LCG\n",
    "min_val = -10     # Minimum value of random numbers\n",
    "max_val = 10      # Maximum value of random numbers\n",
    "\n",
    "# Timing the function\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
    "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llama_hf_on_problem(python_code, output_cpp=\"optimized_llama31.cpp\"):\n",
    "    print(\"=== Generating with Llama-3.1-8B-Instruct ===\")\n",
    "    cpp_code = \"\"\n",
    "    for chunk in stream_llama_hf(python_code):\n",
    "        print(chunk, end='', flush=True)\n",
    "        cpp_code += chunk\n",
    "    print(\"\\n=== Writing and compiling ===\")\n",
    "    write_output(cpp_code, output_cpp)\n",
    "    compile_cmd = [\"g++\", \"-O3\", \"-std=c++17\", \"-o\", \"optimized_llama31.exe\", output_cpp]\n",
    "    compile_result = subprocess.run(compile_cmd, capture_output=True, text=True)\n",
    "    if compile_result.returncode != 0:\n",
    "        print(f\"Compilation error: {compile_result.stderr}\")\n",
    "        return\n",
    "    print(\"=== Running executable ===\")\n",
    "    run_cmd = [\"optimized_llama31.exe\"]\n",
    "    run_result = subprocess.run(run_cmd, capture_output=True, text=True)\n",
    "    print(run_result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d225ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Replace your test cells with these ---\n",
    "print(\"Testing Pi Calculation with Llama-3.1-8B-Instruct\")\n",
    "run_llama_hf_on_problem(pi)\n",
    "\n",
    "print(\"Testing Maximum Subarray Sum with Llama-3.1-8B-Instruct\")\n",
    "run_llama_hf_on_problem(python_hard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
