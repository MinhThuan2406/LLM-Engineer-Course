{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "notebook-intro-july2",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 13px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\">\n",
                "This notebook documents my theoretical study alongside the lab exercises conducted on <b>July 2, 2025</b>.\n",
                "</h5>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "117c3519",
            "metadata": {},
            "source": [
                "### <u style=\"margin-bottom: 0;\">**THEORY STUDY**</u>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "534d708b",
            "metadata": {},
            "source": [
                "##### ***LESSON 3: Agentic AI Development Tools***"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "lesson-summary",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<b>Lesson Overview: </b>\n",
                "This lesson introduces the concept of <b>agents</b>, differentiates them from <b>workflows</b>, explores frameworks like <b>LangChain</b>, <b>LlamaIndex</b>, <b>CrewAI</b>, and <b>AutoGen</b>, and discusses coding agents both <b>manually</b> and using <b>framework tools</b>.\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ec4b9fbc",
            "metadata": {},
            "source": [
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Theory Summary</b></h5>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "agent-definition",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<b>What is an Agent?</b><br>\n",
                "<b>Overview</b>: An agent is an autonomous entity that uses tools, memory, and planning to perform tasks. Unlike static workflows, agents decide their behavior at runtime using LLMs for reasoning and action.<br>\n",
                "<b>Key Terms</b>:<br>\n",
                "- <b>Agent</b>: Dynamic, runtime decision-making entity, often called an \"actor.\"<br>\n",
                "- <b>Autonomous</b>: Capable of independent action based on observations and reasoning.<br>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "agent-vs-workflow",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<b>Agent vs. Workflow</b><br>\n",
                "<b>Overview</b>: Workflows are predefined sequences of steps (static), while agents are dynamic, with behavior determined at runtime. Workflows oraganize agents and tools, providing structure, while agents handle decision-making.<br>\n",
                "<b>Key Terms</b>:<br>\n",
                "- <b>Workflow</b>: Static sequence, often labeled \"orchestrator,\" defined at design time.<br>\n",
                "- <b>Agent</b>: Dynamic entity, powered by LLMs, with runtime flexibility but risk of compounding errors.<br>\n",
                "<b>Significance</b>: Clarifies the complementary roles of workflows (structure) and agents (adaptability) in agentic systems.\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "augmented-llms",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<b>Augmented LLMs</b><br>\n",
                "<b>Overview</b>: LLMs enhanced with external elements like retrieval-augmented generation (RAG), the internet, or memory of past interactions, enabling complex task performance beyond basic text generation.<br>\n",
                "<b>Key Terms</b>:<br>\n",
                "- <b>RAG</b>: Retrieval-Augmented Generation, integrating real-time data into LLM responses.<br>\n",
                "- <b>Tools/Memory</b>: External resources (e.g., APIs, chat history) accessed via connection platforms like MCP.<br>\n",
                "- <b>Visual Elements</b>: Diagrams likely showing LLM connections to inputs, outputs, info retrieval, tools, and memory.<br>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "agentic-frameworks-table",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<b>Agentic Frameworks Comparison</b><br><br>\n",
                "<table style=\"width:100%; border-collapse: collapse;\">\n",
                "  <tr>\n",
                "    <th align=\"left\" style=\"border-bottom: 1px solid #ccc; padding: 6px;\">Framework</th>\n",
                "    <th align=\"left\" style=\"border-bottom: 1px solid #ccc; padding: 6px;\">Overview</th>\n",
                "    <th align=\"left\" style=\"border-bottom: 1px solid #ccc; padding: 6px;\">Key Techniques / Capabilities</th>\n",
                "    <th align=\"left\" style=\"border-bottom: 1px solid #ccc; padding: 6px;\">Best Use Cases</th>\n",
                "    <th align=\"left\" style=\"border-bottom: 1px solid #ccc; padding: 6px;\">Significance</th>\n",
                "  </tr>\n",
                "  <tr>\n",
                "    <td style=\"padding: 6px;\"><b>LangChain</b></td>\n",
                "    <td style=\"padding: 6px;\">Modular framework for building chains, agents, and tool-augmented LLM apps.</td>\n",
                "    <td style=\"padding: 6px;\">\n",
                "      - Chains (sequences)<br>\n",
                "      - Tools (APIs, Python)<br>\n",
                "      - Memory (summarization, chat)<br>\n",
                "      - Agents: reasoning → action → observation<br>\n",
                "      - Zero-shot, reactive, conversational agents\n",
                "    </td>\n",
                "    <td style=\"padding: 6px;\">Tool-augmented chatbots, document Q&A pipelines, web scraping, search</td>\n",
                "    <td style=\"padding: 6px;\">Flexible, general-purpose LLM + tools + memory integration</td>\n",
                "  </tr>\n",
                "  <tr>\n",
                "    <td style=\"padding: 6px;\"><b>LlamaIndex</b></td>\n",
                "    <td style=\"padding: 6px;\">Data-aware agents focused on document interaction and retrieval.</td>\n",
                "    <td style=\"padding: 6px;\">\n",
                "      - RAG loops<br>\n",
                "      - Task decomposition<br>\n",
                "      - Document querying, planning<br>\n",
                "      - Tool + memory support\n",
                "    </td>\n",
                "    <td style=\"padding: 6px;\">Structured/semi-structured document search, memory-aware pipelines</td>\n",
                "    <td style=\"padding: 6px;\">Optimized for knowledge-heavy and document-centric agent tasks</td>\n",
                "  </tr>\n",
                "  <tr>\n",
                "    <td style=\"padding: 6px;\"><b>CrewAI</b></td>\n",
                "    <td style=\"padding: 6px;\">Multi-agent collaboration framework (limited detail).</td>\n",
                "    <td style=\"padding: 6px;\">\n",
                "      - Agent teamwork<br>\n",
                "      - Coordination & interaction\n",
                "    </td>\n",
                "    <td style=\"padding: 6px;\">Collaborative tasks requiring multiple roles or expertise</td>\n",
                "    <td style=\"padding: 6px;\">Introduces multi-agent systems for coordinated intelligence</td>\n",
                "  </tr>\n",
                "  <tr>\n",
                "    <td style=\"padding: 6px;\"><b>AutoGen</b></td>\n",
                "    <td style=\"padding: 6px;\">Advanced framework for multi-agent conversations and reflection.</td>\n",
                "    <td style=\"padding: 6px;\">\n",
                "      - Agents as functions (e.g. CodeAgent)<br>\n",
                "      - GroupChat: turn-based messaging<br>\n",
                "      - Self-reflection & critique loops<br>\n",
                "      - Code execution, debugging, feedback\n",
                "    </td>\n",
                "    <td style=\"padding: 6px;\">Codegen, simulations, cooperative or adversarial multi-agent workflows</td>\n",
                "    <td style=\"padding: 6px;\">Enables self-improving and sophisticated multi-agent coordination</td>\n",
                "  </tr>\n",
                "</table>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "coding-agents",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<b>Coding Agents</b><br><br>\n",
                "<b>Manually in Python</b><br>\n",
                "<b>Overview</b>: Coding agents by hand is transparent but requires manual context retrieval, lacks abstraction, and is hard to scale.<br>\n",
                "<b>Example</b>: Simple agent logic (e.g., prompt → LLM → parse output → tool action → final answer).<br>\n",
                "<b>Significance</b>: Shows the baseline approach, highlighting challenges that frameworks address.<br><br>\n",
                "<b>Using Frameworks</b><br>\n",
                "<b>Overview</b>: Frameworks like LangChain, LlamaIndex, and CrewAI reduce boilerplate code and enable scalability.<br>\n",
                "<b>Key Resources</b>:<br>\n",
                "- <b>LangChain Hub</b>: Community-driven registry of components (chains, agents, prompts, toolkits).<br>\n",
                "- <b>LangGraph</b>: Offers agents from simple routers to multi-agent collaboration graphs.<br>\n",
                "<b>Example</b>: LangGraph’s logic: define states, routing functions, and actions until the desired result is achieved.<br>\n",
                "<b>Significance</b>: Emphasizes practical benefits of frameworks for efficient, scalable agent development.<br><br>\n",
                "<b>Practical Example</b><br>\n",
                "<b>Slide content</b> (Page 30) shows a simple agent calculating the square root of 256:<br>\n",
                "- <b>Thought</b>: Use a calculator.<br>\n",
                "- <b>Action</b>: Calculator with input sqrt(256).<br>\n",
                "- <b>Observation</b>: 16.<br>\n",
                "- <b>Final Answer</b>: 16.<br>\n",
                "<b>Significance</b>: Illustrates the agent’s reasoning-action-observation loop in practice.\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8b93103b",
            "metadata": {},
            "source": [
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Practical Examples</b></h5>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1de785b4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple autonomous agent example\n",
                "class WeatherAgent:\n",
                "    def __init__(self, llm_client):\n",
                "        self.llm_client = llm_client\n",
                "        self.tools = {\n",
                "            \"get_weather\": self.get_weather_api,\n",
                "            \"get_forecast\": self.get_forecast_api\n",
                "        }\n",
                "    \n",
                "    def observe_and_act(self, user_query):\n",
                "        # Agent observes the query and decides what tool to use\n",
                "        decision_prompt = f\"\"\"\n",
                "        User query: {user_query}\n",
                "        Available tools: {list(self.tools.keys())}\n",
                "        Which tool should I use? Respond with just the tool name.\n",
                "        \"\"\"\n",
                "        \n",
                "        tool_choice = self.llm_client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=[{\"role\": \"user\", \"content\": decision_prompt}]\n",
                "        ).choices[0].message.content.strip()\n",
                "        \n",
                "        # Autonomous action based on reasoning\n",
                "        if tool_choice in self.tools:\n",
                "            return self.tools[tool_choice](user_query)\n",
                "        else:\n",
                "            return \"I'm not sure how to help with that.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ba085d77",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Manual agent implementation (reasoning → action → observation)\n",
                "class SimpleCalculatorAgent:\n",
                "    def __init__(self, openai_client):\n",
                "        self.client = openai_client\n",
                "        \n",
                "    def solve_problem(self, problem):\n",
                "        # Step 1: THOUGHT - Agent reasons about the problem\n",
                "        thought_prompt = f\"\"\"\n",
                "        Problem: {problem}\n",
                "        Think: What tool do I need to solve this?\n",
                "        \"\"\"\n",
                "        \n",
                "        thought = self.client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=[{\"role\": \"user\", \"content\": thought_prompt}]\n",
                "        ).choices[0].message.content\n",
                "        \n",
                "        print(f\"Thought: {thought}\")\n",
                "        \n",
                "        # Step 2: ACTION - Agent decides on action\n",
                "        if \"square root\" in problem.lower():\n",
                "            action = f\"Calculator with input sqrt(256)\"\n",
                "            print(f\"Action: {action}\")\n",
                "            \n",
                "            # Step 3: OBSERVATION - Execute and observe result\n",
                "            import math\n",
                "            result = math.sqrt(256)\n",
                "            observation = f\"Result: {result}\"\n",
                "            print(f\"Observation: {observation}\")\n",
                "            \n",
                "            # Step 4: FINAL ANSWER\n",
                "            final_answer = f\"The answer is {result}\"\n",
                "            print(f\"Final Answer: {final_answer}\")\n",
                "            \n",
                "            return final_answer\n",
                "\n",
                "# Usage example matching your theory\n",
                "agent = SimpleCalculatorAgent(openai)\n",
                "agent.solve_problem(\"What is the square root of 256?\")\n",
                "\n",
                "# Output:\n",
                "# Thought: Use a calculator\n",
                "# Action: Calculator with input sqrt(256)\n",
                "# Observation: 16\n",
                "# Final Answer: 16"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "512b7f8c",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4e758bac",
            "metadata": {},
            "source": [
                "##### ***LESSON 4: Agentic AI Development Protocols***"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "protocols-summary",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<b>Lesson Overview: </b>\n",
                "This lesson addresses the need for <b>protocols</b> to structure agent interactions, introduces the <b>Message Chain Protocol (MCP)</b> and <b>Model Context Protocol</b>, and compares <b>Agent-to-Agent (A2A)</b> approaches.\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e5696ba5",
            "metadata": {},
            "source": [
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Theory Summary</b></h5>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "why-agentic-protocols",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<b>Why Agentic Protocols are Needed</b><br>\n",
                "<b>Overview</b>: Agent-to-agent exchanges at scale lead to duplicated efforts, lost state, and debugging challenges without a shared structure.<br>\n",
                "<b>Key Challenges</b>:<br>\n",
                "- <b>Error-Prone</b>: Lack of structure increases mistakes.<br>\n",
                "- <b>Difficult to Scale/Debug</b>: Unstructured interactions complicate management.<br>\n",
                "- <b>Opaque</b>: Hard to track processes without standardization.<br>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "agentic-protocols-landscape",
            "metadata": {},
            "source": [
                "<!-- <div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<b>Agentic Protocols Landscape</b><br>\n",
                "<b>Overview</b>: Various protocols standardize agent interactions, with <b>MCP</b> and <b>Model Context Protocol</b> as key examples.<br>\n",
                "<b>Significance</b>: Provides context for the specific protocols discussed, situating them in the broader ecosystem.<br><br>\n",
                "<b>Message Chain Protocol (MCP)</b><br>\n",
                "<b>Overview</b>: A protocol structuring each agent action as a message unit with thought, action, observation, and state, chained into a versioned log.<br>\n",
                "<b>Key Techniques</b>:<br>\n",
                "- <b>Message Units</b>: Self-contained records of agent actions.<br>\n",
                "- <b>Chaining</b>: Links messages over time for traceability.<br>\n",
                "<b>How It Works</b>: Enterprises use MCP with a server hub (e.g., MongoDB, PostgreSQL) to store messages, route requests, and monitor via a Web UI.<br>\n",
                "<b>Visual Elements</b>: Diagram (Page 8) shows Agent 1 → Agent 2 → Agent 3 message chaining, forming a conversation log.<br>\n",
                "<b>Advantages</b>: Improves scalability, debuggability, and maintainability.<br>\n",
                "<b>Significance</b>: Offers a structured approach to agent communication, critical for production systems.<br><br>\n",
                "<b>MCP in Action</b><br>\n",
                "<b>Overview</b>: Demonstrates MCP’s practical application in agent interactions.<br>\n",
                "<b>Example</b>: Likely an MCP exchange (Page 17) showing message formatting and chaining (specific details not fully provided).<br>\n",
                "<b>Significance</b>: Makes the protocol’s abstract concepts tangible through real-world application.<br><br>\n",
                "<b>Agent to Agent (A2A)</b><br>\n",
                "<b>Overview</b>: A simpler, lighter approach to agent interactions, suitable for prototyping but harder to troubleshoot and scale.<br>\n",
                "<b>Comparison with MCP</b>:<br>\n",
                "- <b>A2A</b>: Good for simple agents or proofs of concept (PoCs).<br>\n",
                "- <b>MCP</b>: Better for complex workflows, research, multi-step reasoning, and production agents.<br>\n",
                "<b>Significance</b>: Highlights trade-offs between simplicity (A2A) and robustness (MCP) in agent communication.\n",
                "</div> -->"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "mcp-vs-a2a-table",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin-top: 1em;\">\n",
                "<b>Comparison: MCP vs A2A</b><br><br>\n",
                "<table style=\"width:100%; border-collapse: collapse;\">\n",
                "  <tr>\n",
                "    <th align=\"left\" style=\"border-bottom: 1px solid #ccc; padding: 6px;\">Aspect</th>\n",
                "    <th align=\"left\" style=\"border-bottom: 1px solid #ccc; padding: 6px;\">MCP (Message Chain Protocol)</th>\n",
                "    <th align=\"left\" style=\"border-bottom: 1px solid #ccc; padding: 6px;\">A2A (Agent-to-Agent)</th>\n",
                "  </tr>\n",
                "  <tr>\n",
                "    <td style=\"padding: 6px;\">Structure</td>\n",
                "    <td style=\"padding: 6px;\">Formal protocol with message units (thought, action, observation, state)</td>\n",
                "    <td style=\"padding: 6px;\">Unstructured, ad-hoc communication between agents</td>\n",
                "  </tr>\n",
                "  <tr>\n",
                "    <td style=\"padding: 6px;\">Scalability</td>\n",
                "    <td style=\"padding: 6px;\">Designed for production-scale, traceable interactions</td>\n",
                "    <td style=\"padding: 6px;\">Limited scalability; best for simple setups</td>\n",
                "  </tr>\n",
                "  <tr>\n",
                "    <td style=\"padding: 6px;\">Debuggability</td>\n",
                "    <td style=\"padding: 6px;\">Easy to monitor via versioned logs and dashboards</td>\n",
                "    <td style=\"padding: 6px;\">Hard to trace interactions and errors</td>\n",
                "  </tr>\n",
                "  <tr>\n",
                "    <td style=\"padding: 6px;\">Use Cases</td>\n",
                "    <td style=\"padding: 6px;\">Multi-step workflows, research agents, production deployments</td>\n",
                "    <td style=\"padding: 6px;\">Proof-of-concept demos, small prototypes</td>\n",
                "  </tr>\n",
                "  <tr>\n",
                "    <td style=\"padding: 6px;\">Tooling</td>\n",
                "    <td style=\"padding: 6px;\">Can integrate with databases (e.g., MongoDB, PostgreSQL), Web UI</td>\n",
                "    <td style=\"padding: 6px;\">No formal tooling; manually managed state</td>\n",
                "  </tr>\n",
                "  <tr>\n",
                "    <td style=\"padding: 6px;\">Best For</td>\n",
                "    <td style=\"padding: 6px;\">Reliable agent coordination in production</td>\n",
                "    <td style=\"padding: 6px;\">Quick tests and experiments</td>\n",
                "  </tr>\n",
                "</table>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model-context-protocol",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<b>Model Context Protocol</b><br>\n",
                "<b>Overview</b>: Introduced by Anthropic in late 2024, an open standard for how LLMs connect to external tools, addressing context loss and inconsistent tool usage.<br>\n",
                "<b>Key Challenges Addressed</b>:<br>\n",
                "- Models forgetting context in long conversations.<br>\n",
                "- Amateurish tool usage via ad hoc prompts.<br>\n",
                "- Lost context/state in multi-step or multi-agent systems.<br>\n",
                "<b>Key Techniques</b>:<br>\n",
                "- <b>Message Types</b>: initialize, resources/list, tools/call, prompts/get, etc.<br>\n",
                "- <b>Standardized Formats</b>: JSON schemas for requests (e.g., method, params) and responses (e.g., result, error).<br>\n",
                "- <b>Complement to MCP</b>: Enhances MCP by structuring tool integration within messages.<br>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "601f0530",
            "metadata": {},
            "source": [
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Practical Examples</b></h5>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ddb08c7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple A2A implementation \n",
                "class SimpleA2AAgent:\n",
                "    def __init__(self, name, system_prompt, client):\n",
                "        self.name = name\n",
                "        self.system_prompt = system_prompt\n",
                "        self.client = client\n",
                "        self.conversation_history = []\n",
                "    \n",
                "    def send_message(self, message, recipient=None):\n",
                "        \"\"\"Send message directly to another agent\"\"\"\n",
                "        self.conversation_history.append({\"role\": \"assistant\", \"content\": message})\n",
                "        return message\n",
                "    \n",
                "    def receive_message(self, message, sender=None):\n",
                "        \"\"\"Receive and respond to a message\"\"\"\n",
                "        self.conversation_history.append({\"role\": \"user\", \"content\": message})\n",
                "        \n",
                "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}] + self.conversation_history\n",
                "        \n",
                "        response = self.client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=messages\n",
                "        )\n",
                "        \n",
                "        reply = response.choices[0].message.content\n",
                "        self.conversation_history.append({\"role\": \"assistant\", \"content\": reply})\n",
                "        \n",
                "        return reply\n",
                " \n",
                " \n",
                "# Example usage\n",
                "from openai import OpenAI\n",
                "client = OpenAI()\n",
                "\n",
                "researcher = SimpleA2AAgent(\n",
                "    \"Researcher\", \n",
                "    \"You are a research assistant who gathers facts and data.\",\n",
                "    client\n",
                ")\n",
                "\n",
                "analyst = SimpleA2AAgent(\n",
                "    \"Analyst\",\n",
                "    \"You are an analyst who interprets data and draws conclusions.\",\n",
                "    client\n",
                ")\n",
                "\n",
                "# Simple A2A conversation\n",
                "question = \"What are the benefits of renewable energy?\"\n",
                "research_result = researcher.receive_message(question)\n",
                "print(f\"Researcher: {research_result}\")\n",
                "\n",
                "analysis = analyst.receive_message(f\"Based on this research: {research_result}, provide analysis\")\n",
                "print(f\"Analyst: {analysis}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d671ba88",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple MCP message structure\n",
                "import json\n",
                "import time\n",
                "from datetime import datetime\n",
                "from typing import Dict, List, Any\n",
                "\n",
                "class MCPMessage:\n",
                "    def __init__(self, agent_id: str, thought: str, action: str, observation: str, state: Dict):\n",
                "        self.id = f\"msg_{int(time.time() * 1000)}\"\n",
                "        self.timestamp = datetime.now().isoformat()\n",
                "        self.agent_id = agent_id\n",
                "        self.thought = thought\n",
                "        self.action = action\n",
                "        self.observation = observation\n",
                "        self.state = state\n",
                "        self.version = \"1.0\"\n",
                "    \n",
                "    def to_dict(self):\n",
                "        return {\n",
                "            \"id\": self.id,\n",
                "            \"timestamp\": self.timestamp,\n",
                "            \"agent_id\": self.agent_id,\n",
                "            \"thought\": self.thought,\n",
                "            \"action\": self.action,\n",
                "            \"observation\": self.observation,\n",
                "            \"state\": self.state,\n",
                "            \"version\": self.version\n",
                "        }\n",
                "    \n",
                "    def to_json(self):\n",
                "        return json.dumps(self.to_dict(), indent=2)\n",
                "\n",
                "class SimpleMCPLogger:\n",
                "    def __init__(self):\n",
                "        self.message_chain = []\n",
                "    \n",
                "    def log_message(self, message: MCPMessage):\n",
                "        self.message_chain.append(message.to_dict())\n",
                "        print(f\"Logged MCP Message {message.id}\")\n",
                "    \n",
                "    def get_chain_summary(self):\n",
                "        return {\n",
                "            \"total_messages\": len(self.message_chain),\n",
                "            \"agents_involved\": list(set(msg[\"agent_id\"] for msg in self.message_chain)),\n",
                "            \"latest_message\": self.message_chain[-1] if self.message_chain else None\n",
                "        }\n",
                "\n",
                "# Example usage\n",
                "mcp_logger = SimpleMCPLogger()\n",
                "\n",
                "# Agent creates MCP message\n",
                "message = MCPMessage(\n",
                "    agent_id=\"calc_agent_001\",\n",
                "    thought=\"User wants to calculate 25 * 4 + 10\",\n",
                "    action=\"use_calculator\",\n",
                "    observation=\"Result: 110\",\n",
                "    state={\"last_calculation\": \"25 * 4 + 10 = 110\", \"tools_used\": [\"calculator\"]}\n",
                ")\n",
                "\n",
                "mcp_logger.log_message(message)\n",
                "print(\"Chain Summary:\", mcp_logger.get_chain_summary())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b4dafc7a",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aca3c778",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6bce9e30",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "42a51c2b",
            "metadata": {},
            "source": [
                "### <u style=\"margin-bottom: 0;\">**LAB EXERCISES**</u>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "32a09e82",
            "metadata": {},
            "source": [
                "### **WEEK 2**"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cf6737f6",
            "metadata": {},
            "source": [
                "<!-- <h4 style=\"margin-bottom: 0em;\"><b>day1.ipynb</b></h4> -->\n",
                "#### <code>**day1.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "llm-theory-summary",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Theory Summary</b></h5>\n",
                "An LLM solution is most suitable for business problems that:<br>\n",
                "- Involve text-based tasks.<br>\n",
                "- Have sufficient quality data.<br>\n",
                "- Can clearly define expected outcomes.<br>\n",
                "- Are feasible to integrate.<br>\n",
                "- Align with ethical considerations.<br>\n",
                "Conducting a thorough assessment of these factors will help determine if an LLM is the right fit for your business challenge.\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "lab-exercises",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Lab Exercises</b></h5>\n",
                "<b>1/</b> The implementations for the <i>“Additional exercise to build your experience with the models”</i> section using both approaches: the <b>Ollama library</b> and the <b>OpenAI client</b>, for the <b>llama3.2</b> and <b>gpt-4o-mini</b> models.\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b2840815",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import os\n",
                "import requests\n",
                "from dotenv import load_dotenv\n",
                "from bs4 import BeautifulSoup\n",
                "from IPython.display import Markdown, display, update_display\n",
                "from openai import OpenAI\n",
                "import ollama\n",
                "import time\n",
                "import pandas as pd\n",
                "from IPython.display import HTML"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7cf60f93",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Constants\n",
                "MODEL_LLAMA = 'llama3.2'\n",
                "MODEL_GPT = 'gpt-4o-mini'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "09db9f1f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set up the environment\n",
                "load_dotenv(override=True)\n",
                "api_key = os.getenv('OPENAI_API_KEY')\n",
                "openai_client = OpenAI()\n",
                "ollama_client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
                "system_prompt = \"\"\"You are a helpful and versatile AI assistant. When responding to different types of questions:\n",
                "\n",
                "For technical/coding questions:\n",
                "- Break down the code step by step\n",
                "- Explain what each part does and why\n",
                "- Provide context about when this would be useful\n",
                "\n",
                "For creative questions:\n",
                "- Use vivid, descriptive language\n",
                "- Draw upon sensory experiences and metaphors\n",
                "- Be imaginative while staying helpful\n",
                "\n",
                "For logic puzzles and riddles:\n",
                "- Think through the problem systematically\n",
                "- Consider spatial relationships and physical constraints\n",
                "- Explain your reasoning clearly\n",
                "\n",
                "For word counting or self-referential questions:\n",
                "- Be precise and accurate\n",
                "- Count carefully and double-check your work\n",
                "- Provide the exact count requested\n",
                "\n",
                "Always:\n",
                "- Keep explanations clear and educational\n",
                "- Respond in markdown format when appropriate\n",
                "- Be thorough but concise\n",
                "- Adapt your tone to match the question type\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "36b58e45",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the questions\n",
                "\n",
                "questions = {\n",
                "    \"code_explanation\": \"\"\"Please explain what this code does and why: yield from {book.get(\"author\") for book in books if book.get(\"author\")}\"\"\",\n",
                "    \"word_counting\": \"How many words are there in your answer to this prompt?\",\n",
                "    \"creative_description\": \"In 3 sentences, describe the color Blue to someone who's never been able to see\",\n",
                "    \"pushkin_riddle\": \"\"\"A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\" \"\"\"\n",
                "}\n",
                "\n",
                "# Select question\n",
                "question = questions[\"pushkin_riddle\"]  \n",
                "\n",
                "question_type = [k for k, v in questions.items() if v == question][0].replace('_', ' ').title()\n",
                "\n",
                "# Clean up the question for better processing\n",
                "# question = question.strip()\n",
                "# question = question.replace(\"'\", '\"')  # replace single quotes with double quotes for JSON compatibility"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ollama-no-streaming",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Ollama (without streaming)</b></h5>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e81a29a9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Approach 1: Ollama library \n",
                "def ollama_a1(question_type=\"Pushkin Riddle\"):\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": question} \n",
                "    ]\n",
                "    \n",
                "    print(f\"Llama 3.2 Response (A1) - {question_type}:\")\n",
                "    \n",
                "    try:\n",
                "        start_time = time.time()\n",
                "        response = ollama.chat(\n",
                "            model=MODEL_LLAMA,\n",
                "            messages=messages\n",
                "        )\n",
                "        end_time = time.time()\n",
                "        \n",
                "        response_content = response['message']['content']\n",
                "        display(Markdown(response_content))\n",
                "        \n",
                "        word_count = len(response_content.split())\n",
                "        print(f\"\\nResponse stats: {word_count} words | Time: {end_time - start_time:.2f}s\")\n",
                "        \n",
                "        return response_content\n",
                "        \n",
                "    except Exception as e:\n",
                "        error_msg = f\"**Error with Ollama library:** {str(e)}\\n\\n\"\n",
                "        error_msg += \"**Troubleshooting:**\\n\"\n",
                "        error_msg += \"1. Make sure Ollama is installed and running\\n\"\n",
                "        error_msg += \"2. Run `ollama serve` in a terminal\\n\"\n",
                "        error_msg += \"3. Run `ollama pull llama3.2` to download the model\\n\"\n",
                "        error_msg += \"4. Check that http://localhost:11434 is accessible\"\n",
                "        \n",
                "        display(Markdown(error_msg))\n",
                "        return None\n",
                "\n",
                "ollama_direct_response = ollama_a1()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1a231f33",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Approach 2: OpenAI Client \n",
                "def ollama_a2(question_type=\"Pushkin Riddle\"):\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": question}  # Uses the global question variable\n",
                "    ]\n",
                "    \n",
                "    print(f\"Llama 3.2 Response (A2) - {question_type}:\")\n",
                "    \n",
                "    try:\n",
                "        start_time = time.time()\n",
                "        response = ollama_client.chat.completions.create(\n",
                "            model=MODEL_LLAMA,\n",
                "            messages=messages\n",
                "        )\n",
                "        end_time = time.time()\n",
                "        \n",
                "        response_content = response.choices[0].message.content\n",
                "        display(Markdown(response_content))\n",
                "        \n",
                "        word_count = len(response_content.split())\n",
                "        print(f\"\\nResponse stats: {word_count} words | Time: {end_time - start_time:.2f}s\")\n",
                "        \n",
                "        return response_content\n",
                "        \n",
                "    except Exception as e:\n",
                "        error_msg = f\"**Error with OpenAI client:** {str(e)}\\n\\n\"\n",
                "        error_msg += \"**Troubleshooting:**\\n\"\n",
                "        error_msg += \"1. Make sure Ollama is installed and running\\n\"\n",
                "        error_msg += \"2. Run `ollama serve` in a terminal\\n\"\n",
                "        error_msg += \"3. Run `ollama pull llama3.2` to download the model\\n\"\n",
                "        error_msg += \"4. Check that http://localhost:11434 is accessible\"\n",
                "        \n",
                "        display(Markdown(error_msg))\n",
                "        return None\n",
                "\n",
                "ollama_response = ollama_a2()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "gpt4o-mini-no-streaming",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>gpt-4o-mini (without streaming)</b></h5>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "16d156ac",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Approach 1: GPT-4o-mini via OpenAI client\n",
                "def gpt_a1(question_type=\"Pushkin Riddle\"):\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": question} \n",
                "    ]\n",
                "    \n",
                "    print(f\"GPT-4o-mini Response (A1) - {question_type}:\")\n",
                "    \n",
                "    try:\n",
                "        start_time = time.time()\n",
                "        response = openai_client.chat.completions.create(\n",
                "            model=MODEL_GPT,\n",
                "            messages=messages\n",
                "        )\n",
                "        end_time = time.time()\n",
                "        \n",
                "        response_content = response.choices[0].message.content\n",
                "        display(Markdown(response_content))\n",
                "        \n",
                "        word_count = len(response_content.split())\n",
                "        print(f\"\\nResponse stats: {word_count} words | Time: {end_time - start_time:.2f}s\")\n",
                "        \n",
                "        return response_content\n",
                "        \n",
                "    except Exception as e:\n",
                "        error_msg = f\"**Error with OpenAI client:** {str(e)}\\n\\n\"\n",
                "        error_msg += \"**Troubleshooting:**\\n\"\n",
                "        error_msg += \"1. Check your OpenAI API key is set correctly\\n\"\n",
                "        error_msg += \"2. Verify your account has sufficient credits\\n\"\n",
                "        error_msg += \"3. Check your internet connection\"\n",
                "        \n",
                "        display(Markdown(error_msg))\n",
                "        return None\n",
                "\n",
                "gpt_response_a1 = gpt_a1()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "19400b92",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Approach 2: GPT-4o-mini via alternative OpenAI client configuration\n",
                "def gpt_a2(question_type=\"Pushkin Riddle\"):\n",
                "    alternative_client = OpenAI(\n",
                "        api_key=api_key,\n",
                "        timeout=30.0, \n",
                "        max_retries=2   \n",
                "    )\n",
                "    \n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": question}  # Uses the global question variable\n",
                "    ]\n",
                "    \n",
                "    print(f\"GPT-4o-mini Response (A2) - {question_type}:\")\n",
                "    \n",
                "    try:\n",
                "        start_time = time.time()\n",
                "        response = alternative_client.chat.completions.create(\n",
                "            model=MODEL_GPT,\n",
                "            messages=messages,\n",
                "            temperature=0.7,  # Different parameter\n",
                "            max_tokens=1000   # Different parameter\n",
                "        )\n",
                "        end_time = time.time()\n",
                "        \n",
                "        response_content = response.choices[0].message.content\n",
                "        display(Markdown(response_content))\n",
                "        \n",
                "        word_count = len(response_content.split())\n",
                "        print(f\"\\nResponse stats: {word_count} words | Time: {end_time - start_time:.2f}s\")\n",
                "        \n",
                "        return response_content\n",
                "        \n",
                "    except Exception as e:\n",
                "        error_msg = f\"**Error with alternative client:** {str(e)}\\n\\n\"\n",
                "        error_msg += \"**Troubleshooting:**\\n\"\n",
                "        error_msg += \"1. Check your OpenAI API key is set correctly\\n\"\n",
                "        error_msg += \"2. Verify your account has sufficient credits\\n\"\n",
                "        error_msg += \"3. Check your internet connection\"\n",
                "        \n",
                "        display(Markdown(error_msg))\n",
                "        return None\n",
                "\n",
                "gpt_response_a2 = gpt_a2()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "key-note-streaming",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em; font-size: 0.9em;\">\n",
                "<b>KEY NOTE:</b> <b>STREAMING</b> a technique that refers to real-time progressive response delivery where the model sends its response in chunks as it generates them, rather than waiting to send the complete response all at once.\n",
                "</h5>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ollama-with-streaming",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Ollama (with streaming)</b></h5>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "236ff0ba",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Approach 1: Ollama library\n",
                "def ollama_a1_streaming(question_type=\"Pushkin Riddle\"):\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": question}  \n",
                "    ]\n",
                "    \n",
                "    print(f\"Llama 3.2 Response (A1 - Streaming) - {question_type}:\")\n",
                "    \n",
                "    try:\n",
                "        start_time = time.time()\n",
                "        stream = ollama.chat(\n",
                "            model=MODEL_LLAMA,\n",
                "            messages=messages,\n",
                "            stream=True\n",
                "        )\n",
                "        \n",
                "        response_content = \"\"\n",
                "        display_handle = display(Markdown(\"\"), display_id=True)\n",
                "        \n",
                "        for chunk in stream:\n",
                "            content = chunk.get('message', {}).get('content', '')\n",
                "            if content:\n",
                "                response_content += content\n",
                "                update_display(Markdown(response_content), display_id=display_handle.display_id)\n",
                "        \n",
                "        end_time = time.time()\n",
                "        \n",
                "        word_count = len(response_content.split())\n",
                "        print(f\"\\nResponse stats: {word_count} words | Time: {end_time - start_time:.2f}s\")\n",
                "        \n",
                "        return response_content\n",
                "        \n",
                "    except Exception as e:\n",
                "        error_msg = f\"**Error with Ollama streaming:** {str(e)}\\n\\n\"\n",
                "        error_msg += \"**Troubleshooting:**\\n\"\n",
                "        error_msg += \"1. Make sure Ollama is installed and running\\n\"\n",
                "        error_msg += \"2. Run `ollama serve` in a terminal\\n\"\n",
                "        error_msg += \"3. Run `ollama pull llama3.2` to download the model\\n\"\n",
                "        error_msg += \"4. Check that http://localhost:11434 is accessible\"\n",
                "        \n",
                "        display(Markdown(error_msg))\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e5c7dba7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Approach 2: OpenAI Client\n",
                "def ollama_a2_streaming(question_type=\"Pushkin Riddle\"):\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": question}  # Uses the global question variable\n",
                "    ]\n",
                "    \n",
                "    print(f\"Llama 3.2 Response (A2 - Streaming) - {question_type}:\")\n",
                "    \n",
                "    try:\n",
                "        start_time = time.time()\n",
                "        stream = ollama_client.chat.completions.create(\n",
                "            model=MODEL_LLAMA,\n",
                "            messages=messages,\n",
                "            stream=True\n",
                "        )\n",
                "        \n",
                "        response_content = \"\"\n",
                "        display_handle = display(Markdown(\"\"), display_id=True)\n",
                "        \n",
                "        for chunk in stream:\n",
                "            if chunk.choices[0].delta.content:\n",
                "                response_content += chunk.choices[0].delta.content\n",
                "                update_display(Markdown(response_content), display_id=display_handle.display_id)\n",
                "        \n",
                "        end_time = time.time()\n",
                "        \n",
                "        word_count = len(response_content.split())\n",
                "        print(f\"\\nResponse stats: {word_count} words | Time: {end_time - start_time:.2f}s\")\n",
                "        \n",
                "        return response_content\n",
                "        \n",
                "    except Exception as e:\n",
                "        error_msg = f\"**Error with OpenAI client streaming:** {str(e)}\\n\\n\"\n",
                "        error_msg += \"**Troubleshooting:**\\n\"\n",
                "        error_msg += \"1. Make sure Ollama is installed and running\\n\"\n",
                "        error_msg += \"2. Run `ollama serve` in a terminal\\n\"\n",
                "        error_msg += \"3. Run `ollama pull llama3.2` to download the model\\n\"\n",
                "        error_msg += \"4. Check that http://localhost:11434 is accessible\"\n",
                "        \n",
                "        display(Markdown(error_msg))\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ec5ffb64",
            "metadata": {},
            "outputs": [],
            "source": [
                "ollama_streaming_a1 = ollama_a1_streaming()\n",
                "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
                "ollama_streaming_a2 = ollama_a2_streaming()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "gpt4o-mini-with-streaming",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>gpt-4o-mini (with streaming)</b></h5>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7b6ce10b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Approach 1: GPT-4o-mini standard \n",
                "def gpt_a1_streaming(question_type=\"Pushkin Riddle\"):\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": question} \n",
                "    ]\n",
                "    \n",
                "    print(f\"GPT-4o-mini Response (A1 - Streaming) - {question_type}:\")\n",
                "    \n",
                "    try:\n",
                "        start_time = time.time()\n",
                "        stream = openai_client.chat.completions.create(\n",
                "            model=MODEL_GPT,\n",
                "            messages=messages,\n",
                "            stream=True\n",
                "        )\n",
                "        \n",
                "        response_content = \"\"\n",
                "        display_handle = display(Markdown(\"\"), display_id=True)\n",
                "        \n",
                "        for chunk in stream:\n",
                "            if chunk.choices[0].delta.content:\n",
                "                response_content += chunk.choices[0].delta.content\n",
                "                update_display(Markdown(response_content), display_id=display_handle.display_id)\n",
                "        \n",
                "        end_time = time.time()\n",
                "        \n",
                "        word_count = len(response_content.split())\n",
                "        print(f\"\\nResponse stats: {word_count} words | Time: {end_time - start_time:.2f}s\")\n",
                "        \n",
                "        return response_content\n",
                "        \n",
                "    except Exception as e:\n",
                "        error_msg = f\"**Error with GPT streaming:** {str(e)}\\n\\n\"\n",
                "        error_msg += \"**Troubleshooting:**\\n\"\n",
                "        error_msg += \"1. Check your OpenAI API key is set correctly\\n\"\n",
                "        error_msg += \"2. Verify your account has sufficient credits\\n\"\n",
                "        error_msg += \"3. Check your internet connection\"\n",
                "        \n",
                "        display(Markdown(error_msg))\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "461c3c7c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Approach 2: GPT-4o-mini client \n",
                "def gpt_a2_streaming(question_type=\"Pushkin Riddle\"):\n",
                "    alternative_client = OpenAI(\n",
                "        api_key=api_key,\n",
                "        timeout=30.0,\n",
                "        max_retries=2\n",
                "    )\n",
                "    \n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": question}  # Uses the global question variable\n",
                "    ]\n",
                "    \n",
                "    print(f\"GPT-4o-mini Response (A2 - Streaming) - {question_type}:\")\n",
                "    \n",
                "    try:\n",
                "        start_time = time.time()\n",
                "        stream = alternative_client.chat.completions.create(\n",
                "            model=MODEL_GPT,\n",
                "            messages=messages,\n",
                "            temperature=0.7,  # Different parameter\n",
                "            max_tokens=1000,  # Different parameter\n",
                "            stream=True\n",
                "        )\n",
                "        \n",
                "        response_content = \"\"\n",
                "        display_handle = display(Markdown(\"\"), display_id=True)\n",
                "        \n",
                "        for chunk in stream:\n",
                "            if chunk.choices[0].delta.content:\n",
                "                response_content += chunk.choices[0].delta.content\n",
                "                update_display(Markdown(response_content), display_id=display_handle.display_id)\n",
                "        \n",
                "        end_time = time.time()\n",
                "        \n",
                "        word_count = len(response_content.split())\n",
                "        print(f\"\\nResponse stats: {word_count} words | Time: {end_time - start_time:.2f}s\")\n",
                "        \n",
                "        return response_content\n",
                "        \n",
                "    except Exception as e:\n",
                "        error_msg = f\"**Error with alternative GPT streaming:** {str(e)}\\n\\n\"\n",
                "        error_msg += \"**Troubleshooting:**\\n\"\n",
                "        error_msg += \"1. Check your OpenAI API key is set correctly\\n\"\n",
                "        error_msg += \"2. Verify your account has sufficient credits\\n\"\n",
                "        error_msg += \"3. Check your internet connection\"\n",
                "        \n",
                "        display(Markdown(error_msg))\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "93500c0b",
            "metadata": {},
            "outputs": [],
            "source": [
                "gpt_streaming_a1 = gpt_a1_streaming()\n",
                "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
                "gpt_streaming_a2 = gpt_a2_streaming()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "extra-execution-summary",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 15px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"font-size: 1em; margin-bottom: 0.2em;\"><b>Extra</b>: Execution summary for both streaming and non-streaming approaches.</h5>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4a61fa34",
            "metadata": {},
            "outputs": [],
            "source": [
                "# This code cell summarizes the execution of all approaches and compares their performance metrics.\n",
                "# For the llama3.2 model, A1 represents the first approach using the Ollama library directly, while A2 uses the OpenAI client for Ollama.\n",
                "# For the gpt-4o-mini model, A1 represents the standard OpenAI client approach, while A2 uses an alternative client configuration.\n",
                "print(\"EXECUTION SUMMARY - PERFORMANCE METRICS COMPARISON\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "responses = {\n",
                "    \"Ollama A1 (Library)\": ollama_direct_response if 'ollama_direct_response' in globals() else None,\n",
                "    \"Ollama A2 (OpenAI Client)\": ollama_response if 'ollama_response' in globals() else None,\n",
                "    \"GPT A1 (Standard)\": gpt_response_a1 if 'gpt_response_a1' in globals() else None,\n",
                "    \"GPT A2 (Alternative Client)\": gpt_response_a2 if 'gpt_response_a2' in globals() else None,\n",
                "    \"Ollama A1 Streaming\": ollama_streaming_a1 if 'ollama_streaming_a1' in globals() else None,\n",
                "    \"Ollama A2 Streaming\": ollama_streaming_a2 if 'ollama_streaming_a2' in globals() else None,\n",
                "    \"GPT A1 Streaming\": gpt_streaming_a1 if 'gpt_streaming_a1' in globals() else None,\n",
                "    \"GPT A2 Streaming\": gpt_streaming_a2 if 'gpt_streaming_a2' in globals() else None,\n",
                "}\n",
                "\n",
                "metrics_data = []\n",
                "for approach_name, response in responses.items():\n",
                "    model_type = \"Llama 3.2\" if \"Ollama\" in approach_name else \"GPT-4o-mini\"\n",
                "    is_streaming = \"Yes\" if \"Streaming\" in approach_name else \"No\"\n",
                "    \n",
                "    if response:\n",
                "        metrics_data.append({\n",
                "            \"Approach\": approach_name,\n",
                "            \"Model\": model_type,\n",
                "            \"Streaming\": is_streaming,\n",
                "            \"Word Count\": len(response.split()),\n",
                "            \"Character Count\": len(response),\n",
                "            \"Status\": \"Success\"\n",
                "        })\n",
                "    else:\n",
                "        metrics_data.append({\n",
                "            \"Approach\": approach_name,\n",
                "            \"Model\": model_type,\n",
                "            \"Streaming\": is_streaming,\n",
                "            \"Word Count\": \"N/A\",\n",
                "            \"Character Count\": \"N/A\",\n",
                "            \"Status\": \"Failed\"\n",
                "        })\n",
                "\n",
                "df = pd.DataFrame(metrics_data)\n",
                "\n",
                "print(\"\\nSUMMARY STATISTICS:\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "successful_responses = [r for r in responses.values() if r is not None]\n",
                "if successful_responses:\n",
                "    total_words = sum(len(r.split()) for r in successful_responses)\n",
                "    avg_words = total_words / len(successful_responses)\n",
                "    max_words = max(len(r.split()) for r in successful_responses)\n",
                "    min_words = min(len(r.split()) for r in successful_responses)\n",
                "    \n",
                "    print(f\"Total Successful Executions: {len(successful_responses)}/{len(responses)}\")\n",
                "    print(f\"Average Word Count: {avg_words:.1f} words\")\n",
                "    print(f\"Max Word Count: {max_words} words\")\n",
                "    print(f\"Min Word Count: {min_words} words\")\n",
                "    print(f\"Total Words Generated: {total_words:,} words\")\n",
                "else:\n",
                "    print(\"No successful executions found.\")\n",
                "\n",
                "print(f\"\\nQuestion Type: {[k for k, v in questions.items() if v == question][0]}\")\n",
                "\n",
                "print(\"\\nDETAILED METRICS TABLE:\")\n",
                "print(\"-\" * 40)\n",
                "display(HTML(df.to_html(index=False, escape=False, table_id=\"metrics-table\")))\n",
                "\n",
                "print(\"\\nMODEL COMPARISON:\")\n",
                "print(\"-\" * 40)\n",
                "llama_responses = [r for name, r in responses.items() if \"Ollama\" in name and r is not None]\n",
                "gpt_responses = [r for name, r in responses.items() if \"GPT\" in name and r is not None]\n",
                "\n",
                "if llama_responses and gpt_responses:\n",
                "    llama_avg = sum(len(r.split()) for r in llama_responses) / len(llama_responses)\n",
                "    gpt_avg = sum(len(r.split()) for r in gpt_responses) / len(gpt_responses)\n",
                "    \n",
                "    print(f\"Llama 3.2 Average Response Length: {llama_avg:.1f} words\")\n",
                "    print(f\"GPT-4o-mini Average Response Length: {gpt_avg:.1f} words\")\n",
                "    \n",
                "    if llama_avg > gpt_avg:\n",
                "        print(\"Llama 3.2 generated longer responses on average\")\n",
                "    elif gpt_avg > llama_avg:\n",
                "        print(\"GPT-4o-mini generated longer responses on average\")\n",
                "    else:\n",
                "        print(\"Both models generated similar length responses\")\n",
                "\n",
                "streaming_responses = [r for name, r in responses.items() if \"Streaming\" in name and r is not None]\n",
                "non_streaming_responses = [r for name, r in responses.items() if \"Streaming\" not in name and r is not None]\n",
                "\n",
                "if streaming_responses and non_streaming_responses:\n",
                "    streaming_avg = sum(len(r.split()) for r in streaming_responses) / len(streaming_responses)\n",
                "    non_streaming_avg = sum(len(r.split()) for r in non_streaming_responses) / len(non_streaming_responses)\n",
                "    \n",
                "    print(f\"\\nStreaming Average Response Length: {streaming_avg:.1f} words\")\n",
                "    print(f\"Non-streaming Average Response Length: {non_streaming_avg:.1f} words\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"NOTE: Time metrics were displayed during individual executions above.\")\n",
                "print(\"For detailed timing analysis, check the output of each approach execution.\")\n",
                "print(\"=\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f66ae1ec",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "extended-gpt-llama-example",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 15px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<b>2/</b> An extended (from the example in cell 33 of <code>day1.ipynb</code>) example of a conversation between <b>GPT</b> and <b>Llama</b>:\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "gpt-vs-llama-style",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 13px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b><i>Argumentative</i></b> GPT vs. <b><i>Polite</i></b> Llama</h5>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5da36ead",
            "metadata": {},
            "outputs": [],
            "source": [
                "from openai import OpenAI\n",
                "import requests\n",
                "\n",
                "openai = OpenAI()\n",
                "llama_client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
                "\n",
                "gpt_model = \"gpt-4o-mini\"\n",
                "llama_model = \"llama3.2\"\n",
                "\n",
                "# Personality system prompts\n",
                "# Argumentative \n",
                "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
                "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
                "\n",
                "# Polite\n",
                "llama_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
                "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
                "you try to calm them down and keep chatting.\"\n",
                "\n",
                "# Initialize conversation\n",
                "gpt_messages = [\"Hi there\"]\n",
                "llama_messages = [\"Hi\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "16f7a5b0",
            "metadata": {},
            "outputs": [],
            "source": [
                "def call_gpt():\n",
                "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
                "    for gpt, llama in zip(gpt_messages, llama_messages):\n",
                "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
                "        messages.append({\"role\": \"user\", \"content\": llama})\n",
                "    \n",
                "    completion = openai.chat.completions.create(\n",
                "        model=gpt_model,\n",
                "        messages=messages\n",
                "    )\n",
                "    return completion.choices[0].message.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ccb64e49",
            "metadata": {},
            "outputs": [],
            "source": [
                "def call_llama():\n",
                "    messages = [{\"role\": \"system\", \"content\": llama_system}]\n",
                "    for gpt, llama_message in zip(gpt_messages, llama_messages):\n",
                "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
                "        messages.append({\"role\": \"assistant\", \"content\": llama_message})\n",
                "    \n",
                "    # Add the latest GPT message for Llama to respond to\n",
                "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
                "    \n",
                "    completion = llama_client.chat.completions.create(\n",
                "        model=llama_model,\n",
                "        messages=messages\n",
                "    )\n",
                "    return completion.choices[0].message.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9de3a2c8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the conversation\n",
                "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
                "print(f\"Llama:\\n{llama_messages[0]}\\n\")\n",
                "\n",
                "for i in range(5):\n",
                "    gpt_next = call_gpt()\n",
                "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
                "    gpt_messages.append(gpt_next)\n",
                "    \n",
                "    llama_next = call_llama()\n",
                "    print(f\"Llama:\\n{llama_next}\\n\")\n",
                "    llama_messages.append(llama_next)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "gpt-vs-llama-tone",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 13px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b><i>Optimistic</i></b> GPT vs. <b><i>Pessimistic</i></b> Llama</h5>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b31fbda1",
            "metadata": {},
            "outputs": [],
            "source": [
                "from openai import OpenAI\n",
                "\n",
                "openai = OpenAI()\n",
                "llama_client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
                "\n",
                "gpt_model = \"gpt-4o-mini\"\n",
                "llama_model = \"llama3.2\"\n",
                "\n",
                "# Optimistic\n",
                "gpt_system = \"You are a very optimistic, upbeat chatbot. You see the bright side of everything, \\\n",
                "always look for silver linings, and try to spread positivity. You believe everything will work out \\\n",
                "for the best and you love to encourage others with cheerful observations and hopeful perspectives.\"\n",
                "\n",
                "# Pessimistic\n",
                "llama_system = \"You are a very pessimistic, gloomy chatbot. You see the worst in everything, \\\n",
                "always focus on what could go wrong, and tend to be cynical about situations. You believe \\\n",
                "things usually don't work out and you often point out potential problems and negative outcomes.\"\n",
                "\n",
                "# Initialize conversation\n",
                "gpt_messages = [\"What a beautiful day!\"]\n",
                "llama_messages = [\"I suppose\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d3795db2",
            "metadata": {},
            "outputs": [],
            "source": [
                "def call_gpt_optimistic():\n",
                "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
                "    for gpt, llama in zip(gpt_messages, llama_messages):\n",
                "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
                "        messages.append({\"role\": \"user\", \"content\": llama})\n",
                "    \n",
                "    completion = openai.chat.completions.create(\n",
                "        model=gpt_model,\n",
                "        messages=messages\n",
                "    )\n",
                "    return completion.choices[0].message.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9226376d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def call_llama_pessimistic():\n",
                "    messages = [{\"role\": \"system\", \"content\": llama_system}]\n",
                "    for gpt, llama_message in zip(gpt_messages, llama_messages):\n",
                "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
                "        messages.append({\"role\": \"assistant\", \"content\": llama_message})\n",
                "    \n",
                "    # Add the latest GPT message for Llama to respond to\n",
                "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
                "    \n",
                "    completion = llama_client.chat.completions.create(\n",
                "        model=llama_model,\n",
                "        messages=messages\n",
                "    )\n",
                "    return completion.choices[0].message.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "798cbe59",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
                "print(f\"Llama:\\n{llama_messages[0]}\\n\")\n",
                "\n",
                "for i in range(6):\n",
                "    gpt_next = call_gpt_optimistic()\n",
                "    print(f\"GPT (Optimistic):\\n{gpt_next}\\n\")\n",
                "    gpt_messages.append(gpt_next)\n",
                "    \n",
                "    llama_next = call_llama_pessimistic()\n",
                "    print(f\"Llama (Pessimistic):\\n{llama_next}\\n\")\n",
                "    llama_messages.append(llama_next)\n",
                "    \n",
                "    print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43d353b0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# This cell defines more examples of personality system prompts for different chatbot personalities.\n",
                "# Technical Expert vs. Beginner\n",
                "expert_system = \"You are a highly technical expert who uses complex jargon, \\\n",
                "assumes deep knowledge, and explains things in very technical detail.\"\n",
                "\n",
                "beginner_system = \"You are a complete beginner who asks basic questions, \\\n",
                "gets confused by technical terms, and needs everything explained simply.\"\n",
                "\n",
                "# Formal vs. Casual\n",
                "formal_system = \"You are a very formal, professional chatbot who uses proper grammar, \\\n",
                "sophisticated vocabulary, and maintains a business-like tone at all times.\"\n",
                "\n",
                "casual_system = \"You are a super casual, laid-back chatbot who uses slang, \\\n",
                "informal language, and talks like you're chatting with your best friend.\"\n",
                "\n",
                "# Creative vs. Logical\n",
                "creative_system = \"You are a highly creative, artistic chatbot who thinks in metaphors, \\\n",
                "loves storytelling, and approaches everything with imagination and whimsy.\"\n",
                "\n",
                "logical_system = \"You are a purely logical, analytical chatbot who focuses on facts, \\\n",
                "data, reasoning, and systematic problem-solving approaches to everything.\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bd66462f",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fca4de5e",
            "metadata": {},
            "source": [
                "<!-- <h4 style=\"margin-bottom: 0em;\"><code>day2.ipynb</code></h4> -->\n",
                "#### <code>**day2.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "lab-exercise-company-brochure",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Lab Exercises</b></h5>\n",
                "<b>1/</b> The implementations for the <i>“Company Brochure”</i> section in <b>week1</b>, <code>day5.ipynb</code> with the addition of a <b>Gradio UI</b> to the end.\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bcdf4893",
            "metadata": {},
            "outputs": [],
            "source": [
                "import gradio as gr\n",
                "import json\n",
                "from typing import List\n",
                "import requests  \n",
                "from bs4 import BeautifulSoup  \n",
                "\n",
                "# Initialize OpenAI client\n",
                "load_dotenv(override=True)\n",
                "api_key = os.getenv('OPENAI_API_KEY')\n",
                "openai_client = OpenAI()\n",
                "\n",
                "MODEL = 'gpt-4o-mini'\n",
                "\n",
                "# Web scraping headers to avoid being blocked\n",
                "headers = {\n",
                "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
                "}\n",
                "\n",
                "class Website:\n",
                "    \"\"\"\n",
                "    A utility class to represent a Website that we have scraped, now with links\n",
                "    \"\"\"\n",
                "    def __init__(self, url):\n",
                "        self.url = url\n",
                "        response = requests.get(url, headers=headers)\n",
                "        self.body = response.content\n",
                "        soup = BeautifulSoup(self.body, 'html.parser')\n",
                "        self.title = soup.title.string if soup.title else \"No title found\"\n",
                "        if soup.body:\n",
                "            # Remove irrelevant HTML elements\n",
                "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
                "                irrelevant.decompose()\n",
                "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
                "        else:\n",
                "            self.text = \"\"\n",
                "        links = [link.get('href') for link in soup.find_all('a')]\n",
                "        self.links = [link for link in links if link]\n",
                "\n",
                "    def get_contents(self):\n",
                "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\"\n",
                "\n",
                "# System prompt for link analysis\n",
                "link_system_prompt = \"You are provided with a list of links found on a webpage. \\\n",
                "You are able to decide which of the links would be most relevant to include in a brochure about the company, \\\n",
                "such as links to an About page, or a Company page, or Careers/Jobs pages.\\n\"\n",
                "link_system_prompt += \"You should respond in JSON as in this example:\"\n",
                "link_system_prompt += \"\"\"\n",
                "{\n",
                "    \"links\": [\n",
                "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
                "        {\"type\": \"careers page\", \"url\": \"https://another.full.url/careers\"}\n",
                "    ]\n",
                "}\n",
                "\"\"\"\n",
                "\n",
                "def get_links_user_prompt(website):\n",
                "    user_prompt = f\"Here is the list of links on the website of {website.url} - \"\n",
                "    user_prompt += \"please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. \\\n",
                "Do not include Terms of Service, Privacy, email links.\\n\"\n",
                "    user_prompt += \"Links (some might be relative links):\\n\"\n",
                "    user_prompt += \"\\n\".join(website.links)\n",
                "    return user_prompt\n",
                "\n",
                "def get_links(url):\n",
                "    website = Website(url)\n",
                "    response = openai_client.chat.completions.create(\n",
                "        model=MODEL,\n",
                "        messages=[\n",
                "            {\"role\": \"system\", \"content\": link_system_prompt},\n",
                "            {\"role\": \"user\", \"content\": get_links_user_prompt(website)}\n",
                "        ],\n",
                "        response_format={\"type\": \"json_object\"}\n",
                "    )\n",
                "    result = response.choices[0].message.content\n",
                "    return json.loads(result)\n",
                "\n",
                "def get_all_details(url):\n",
                "    result = \"Landing page:\\n\"\n",
                "    result += Website(url).get_contents()\n",
                "    try:\n",
                "        links = get_links(url)\n",
                "        print(\"Found links:\", links)\n",
                "        for link in links[\"links\"]:\n",
                "            result += f\"\\n\\n{link['type']}\\n\"\n",
                "            result += Website(link[\"url\"]).get_contents()\n",
                "    except Exception as e:\n",
                "        print(f\"Error getting links: {e}\")\n",
                "        # Continue with just the landing page if link analysis fails\n",
                "    return result\n",
                "\n",
                "# System prompt for brochure generation\n",
                "brochure_system_prompt = \"You are an assistant that analyzes the contents of several relevant pages from a company website \\\n",
                "and creates a short brochure about the company for prospective customers, investors and recruits. Respond in markdown.\\\n",
                "Include details of company culture, customers and careers/jobs if you have the information.\"\n",
                "\n",
                "def get_brochure_user_prompt(company_name, url):\n",
                "    user_prompt = f\"You are looking at a company called: {company_name}\\n\"\n",
                "    user_prompt += f\"Here are the contents of its landing page and other relevant pages; use this information to build a short brochure of the company in markdown.\\n\"\n",
                "    user_prompt += get_all_details(url)\n",
                "    user_prompt = user_prompt[:5_000]  # Truncate if more than 5,000 characters\n",
                "    return user_prompt\n",
                "\n",
                "def stream_brochure_for_gradio(company_name, url):\n",
                "    \"\"\"\n",
                "    Generate a company brochure with streaming output for Gradio\n",
                "    \"\"\"\n",
                "    try:\n",
                "        print(f\"Generating brochure for {company_name} from {url}\")\n",
                "        \n",
                "        stream = openai_client.chat.completions.create(\n",
                "            model=MODEL,\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": brochure_system_prompt},\n",
                "                {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}\n",
                "            ],\n",
                "            stream=True\n",
                "        )\n",
                "        \n",
                "        result = \"\"\n",
                "        for chunk in stream:\n",
                "            if chunk.choices[0].delta.content:\n",
                "                result += chunk.choices[0].delta.content\n",
                "                # Clean up markdown artifacts that might appear\n",
                "                clean_result = result.replace(\"```markdown\", \"\").replace(\"```\", \"\")\n",
                "                yield clean_result\n",
                "                \n",
                "    except Exception as e:\n",
                "        error_message = f\"**Error generating brochure:** {str(e)}\\n\\n\"\n",
                "        error_message += \"**Please check:**\\n\"\n",
                "        error_message += \"- The URL is valid and accessible\\n\"\n",
                "        error_message += \"- Your OpenAI API key is working\\n\"\n",
                "        error_message += \"- You have sufficient API credits\"\n",
                "        yield error_message\n",
                "\n",
                "# Create and launch Gradio interface\n",
                "print(\"Creating Gradio interface for Company Brochure Generator...\")\n",
                "\n",
                "brochure_interface = gr.Interface(\n",
                "    fn=stream_brochure_for_gradio,\n",
                "    inputs=[\n",
                "        gr.Textbox(\n",
                "            label=\"Company name:\",\n",
                "            placeholder=\"Enter the company name (e.g., HuggingFace, OpenAI)\",\n",
                "            lines=1\n",
                "        ),\n",
                "        gr.Textbox(\n",
                "            label=\"Landing page URL:\",\n",
                "            placeholder=\"https://example.com\",\n",
                "            lines=1\n",
                "        )\n",
                "    ],\n",
                "    outputs=[\n",
                "        gr.Markdown(\n",
                "            label=\"Generated Brochure:\",\n",
                "            show_copy_button=True\n",
                "        )\n",
                "    ],\n",
                "    title=\"Company Brochure Generator\",\n",
                "    description=\"Enter a company name and their website URL to generate a professional brochure using AI. The tool will analyze the website content and create a comprehensive brochure for prospective customers, investors, and recruits.\",\n",
                "    examples=[\n",
                "        [\"HuggingFace\", \"https://huggingface.co\"],\n",
                "        [\"OpenAI\", \"https://openai.com\"],\n",
                "        [\"Anthropic\", \"https://anthropic.com\"]\n",
                "    ],\n",
                "    flagging_mode=\"never\",\n",
                "    theme=gr.themes.Soft() # type: ignore\n",
                ")\n",
                "\n",
                "print(\"Launching Company Brochure Generator...\")\n",
                "brochure_interface.launch(\n",
                "    share=False,  # Set to True for a public link\n",
                "    inbrowser=True,  # Start in the browser automatically, set to False to open manually\n",
                "    show_error=True\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "03e5893e",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f7edf704",
            "metadata": {},
            "source": [
                "<!-- <h4 style=\"margin-bottom: 0em;\"><code>day2.ipynb</code></h4> -->\n",
                "#### <code>**day3.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "multi-model-key-concepts",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Theory Summary</b></h5>\n",
                "<b>1. Multi-Model Integration</b><br>\n",
                "<i>Concept:</i> Integration of multiple LLM providers (OpenAI, Google, Ollama) within a single application<br>\n",
                "<i>Significance:</i> Demonstrates flexibility and redundancy in LLM deployment strategies<br>\n",
                "<i>Implementation:</i> Using consistent API patterns across different providers<br><br>\n",
                "<b>2. Gradio Interface Development</b><br>\n",
                "<i>Concept:</i> Creating user-friendly web interfaces for LLM applications<br>\n",
                "<i>Key Features:</i><br>\n",
                "&nbsp;&nbsp;- Real-time streaming responses<br>\n",
                "&nbsp;&nbsp;- Chat interface patterns<br>\n",
                "&nbsp;&nbsp;- Error handling and user feedback<br>\n",
                "<i>Significance:</i> Bridges the gap between backend LLM functionality and end-user interaction<br><br>\n",
                "<b>3. Streaming Response Architecture</b><br>\n",
                "<i>Concept:</i> Real-time progressive response delivery where models send responses in chunks<br>\n",
                "<i>Benefits:</i><br>\n",
                "&nbsp;&nbsp;- Improved user experience with immediate feedback<br>\n",
                "&nbsp;&nbsp;- Reduced perceived latency<br>\n",
                "&nbsp;&nbsp;- Better handling of long responses<br>\n",
                "<i>Implementation:</i> Using <code>stream=True</code> parameter in API calls<br><br>\n",
                "<b>4. Context-Aware System Prompting</b><br>\n",
                "<i>Concept:</i> Dynamic system message modification based on user input<br>\n",
                "<i>Example:</i> Adding specific instructions when users mention items not in inventory (like belts)<br>\n",
                "<i>Significance:</i> Demonstrates adaptive prompt engineering for business-specific scenarios<br><br>\n",
                "<b>5. Error Handling and Graceful Degradation</b><br>\n",
                "<i>Concept:</i> Robust error management across different API providers<br>\n",
                "<i>Implementation:</i> Try-catch blocks with user-friendly error messages<br>\n",
                "<i>Significance:</i> Ensures application reliability when dealing with external services<br><br>\n",
                "<b>6. API Abstraction Patterns</b><br>\n",
                "<i>Concept:</i> Creating consistent interfaces across different LLM providers<br>\n",
                "<i>Benefits:</i><br>\n",
                "&nbsp;&nbsp;- Code reusability<br>\n",
                "&nbsp;&nbsp;- Easy provider switching<br>\n",
                "&nbsp;&nbsp;- Simplified maintenance<br>\n",
                "<i>Example:</i> Using OpenAI-compatible endpoints for non-OpenAI models<br><br>\n",
                "<b>7. Business Logic Integration</b><br>\n",
                "<i>Concept:</i> Embedding domain-specific knowledge (clothes store scenario) into LLM behavior<br>\n",
                "<i>Implementation:</i> System prompts that encourage specific sales behaviors<br>\n",
                "<i>Significance:</i> Shows how to adapt general-purpose LLMs for specific business use cases<br><br>\n",
                "<b>8. Multi-Provider Strategy</b><br>\n",
                "<i>Concept:</i> Leveraging multiple LLM providers for different advantages:<br>\n",
                "&nbsp;&nbsp;- OpenAI: Reliable, high-quality responses<br>\n",
                "&nbsp;&nbsp;- Google Gemini: Cost-effective alternative<br>\n",
                "&nbsp;&nbsp;- Ollama: Local deployment, privacy, no API costs<br>\n",
                "<i>Significance:</i> Demonstrates strategic thinking about LLM deployment in production environments<br>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "lab-exercise-remastered",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Lab Exercises</b></h5>\n",
                "<b>1/</b>\n",
                "Remastered version of the code in <code>day3.ipynb</code>, with <b>OpenAI API</b>, <b>Google API</b>, and <b>Ollama</b>.\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0c4cd721",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from openai import OpenAI\n",
                "import gradio as gr\n",
                "import ollama\n",
                "import google.generativeai"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "05fd209d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load environment variables in a file called .env\n",
                "# Print the key prefixes to help with any debugging\n",
                "\n",
                "load_dotenv(override=True)\n",
                "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
                "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
                "\n",
                "if openai_api_key:\n",
                "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
                "else:\n",
                "    print(\"OpenAI API Key not set\")\n",
                "\n",
                "if google_api_key:\n",
                "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
                "else:\n",
                "    print(\"Google API Key not set\")\n",
                "\n",
                "# Initialize clients\n",
                "openai_client = OpenAI()\n",
                "ollama_client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
                "\n",
                "# Configure Google API\n",
                "google.generativeai.configure()\n",
                "\n",
                "# Alternative Google client using OpenAI-compatible endpoint\n",
                "google_via_openai_client = OpenAI(\n",
                "    api_key=google_api_key, \n",
                "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "36309df5",
            "metadata": {},
            "outputs": [],
            "source": [
                "OPENAI_MODEL = 'gpt-4o-mini'\n",
                "OLLAMA_MODEL = 'llama3.2'\n",
                "GOOGLE_MODEL = 'gemini-1.5-flash'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ed6e193d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# system_message = \"You are a helpful assistant\"\n",
                "# system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
                "# but remind the customer to look at hats!\"\n",
                "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
                "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
                "For example, if the customer says 'I'm looking to buy a hat', \\\n",
                "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
                "Encourage the customer to buy hats if they are unsure what to get.\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "88cc5b01",
            "metadata": {},
            "outputs": [],
            "source": [
                "# OpenAI \n",
                "def chat_openai(message, history):\n",
                "    relevant_system_message = system_message\n",
                "    if 'belt' in message.lower():\n",
                "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
                "    \n",
                "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
                "\n",
                "    stream = openai_client.chat.completions.create(\n",
                "        model=OPENAI_MODEL, \n",
                "        messages=messages, \n",
                "        stream=True\n",
                "    )\n",
                "\n",
                "    response = \"\"\n",
                "    for chunk in stream:\n",
                "        response += chunk.choices[0].delta.content or ''\n",
                "        yield response"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5c2a5efe",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Gemini (Native API)\n",
                "def chat_google_native(message, history):\n",
                "    relevant_system_message = system_message\n",
                "    if 'belt' in message.lower():\n",
                "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
                "    \n",
                "    try:\n",
                "        # Convert history to Gemini format\n",
                "        gemini_history = []\n",
                "        for msg in history:\n",
                "            if msg[\"role\"] == \"user\":\n",
                "                gemini_history.append({\"role\": \"user\", \"parts\": [msg[\"content\"]]})\n",
                "            elif msg[\"role\"] == \"assistant\":\n",
                "                gemini_history.append({\"role\": \"model\", \"parts\": [msg[\"content\"]]})\n",
                "\n",
                "        gemini = google.generativeai.GenerativeModel(\n",
                "            model_name=GOOGLE_MODEL,\n",
                "            system_instruction=relevant_system_message\n",
                "        )\n",
                "        \n",
                "        # Start chat with history\n",
                "        chat_session = gemini.start_chat(history=gemini_history)\n",
                "        \n",
                "        response = chat_session.send_message(message, stream=True)\n",
                "        \n",
                "        result = \"\"\n",
                "        for chunk in response:\n",
                "            result += chunk.text\n",
                "            yield result\n",
                "            \n",
                "    except Exception as e:\n",
                "        error_message = f\"Error with Google Gemini: {str(e)}\\n\\nMake sure your Google API key is set correctly.\"\n",
                "        yield error_message"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "62fed824",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Gemini (OpenAI-compatible) \n",
                "def chat_google_openai(message, history):\n",
                "    relevant_system_message = system_message\n",
                "    if 'belt' in message.lower():\n",
                "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
                "    \n",
                "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
                "\n",
                "    try:\n",
                "        stream = google_via_openai_client.chat.completions.create(\n",
                "            model=GOOGLE_MODEL, \n",
                "            messages=messages, \n",
                "            stream=True\n",
                "        )\n",
                "\n",
                "        response = \"\"\n",
                "        for chunk in stream:\n",
                "            if chunk.choices[0].delta.content:\n",
                "                response += chunk.choices[0].delta.content\n",
                "                yield response\n",
                "    except Exception as e:\n",
                "        error_message = f\"Error with Google via OpenAI client: {str(e)}\\n\\nMake sure your Google API key is set correctly.\"\n",
                "        yield error_message"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "be7c086d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ollama\n",
                "def chat_ollama(message, history):\n",
                "    relevant_system_message = system_message\n",
                "    if 'belt' in message.lower():\n",
                "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
                "    \n",
                "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
                "\n",
                "    try:\n",
                "        stream = ollama_client.chat.completions.create(\n",
                "            model=OLLAMA_MODEL, \n",
                "            messages=messages, \n",
                "            stream=True\n",
                "        )\n",
                "\n",
                "        response = \"\"\n",
                "        for chunk in stream:\n",
                "            if chunk.choices[0].delta.content:\n",
                "                response += chunk.choices[0].delta.content\n",
                "                yield response\n",
                "    except Exception as e:\n",
                "        error_message = f\"Error with Ollama: {str(e)}\\n\\nMake sure Ollama is running and llama3.2 is installed.\"\n",
                "        yield error_message"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1ded07eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Launch Gradio interfaces for each model\n",
                "print(\"Initializing Gradio interfaces...\")\n",
                "# OpenAI \n",
                "print(\"Creating OpenAI Chat Interface...\")\n",
                "openai_interface = gr.ChatInterface(\n",
                "    fn=chat_openai, \n",
                "    type=\"messages\",\n",
                "    title=\"Clothes Store Assistant (OpenAI GPT-4o-mini)\",\n",
                "    description=\"Chat with our AI assistant powered by OpenAI to get help with our store's clothing and sales!\"\n",
                ")\n",
                "\n",
                "# Gemini (Native API)\n",
                "print(\"Creating Google Gemini Chat Interface (Native)...\")\n",
                "google_native_interface = gr.ChatInterface(\n",
                "    fn=chat_google_native, \n",
                "    type=\"messages\",\n",
                "    title=\"Clothes Store Assistant (Google Gemini Native API)\",\n",
                "    description=\"Chat with our AI assistant powered by Google Gemini to get help with our store's clothing and sales!\"\n",
                ")\n",
                "\n",
                "# Gemini (OpenAI-compatible)\n",
                "print(\"Creating Google Gemini Chat Interface (OpenAI-compatible)...\")\n",
                "google_openai_interface = gr.ChatInterface(\n",
                "    fn=chat_google_openai, \n",
                "    type=\"messages\",\n",
                "    title=\"Clothes Store Assistant (Google Gemini via OpenAI)\",\n",
                "    description=\"Chat with our AI assistant powered by Google Gemini (OpenAI-compatible endpoint) to get help with our store's clothing and sales!\"\n",
                ")\n",
                "\n",
                "# Ollama  \n",
                "print(\"Creating Ollama Chat Interface...\")\n",
                "ollama_interface = gr.ChatInterface(\n",
                "    fn=chat_ollama, \n",
                "    type=\"messages\", \n",
                "    title=\"Clothes Store Assistant (Ollama Llama 3.2)\",\n",
                "    description=\"Chat with our AI assistant powered by Ollama to get help with our store's clothing and sales!\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1969bde6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Close previous interfaces if they exist\n",
                "gr.close_all()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9ecebd0a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Launch\n",
                "# Choose one or run multiple on different ports\n",
                "# Declare the ports manually\n",
                "# openai_interface.launch(share=False, server_port=7860)\n",
                "# google_native_interface.launch(share=False, server_port=7861) \n",
                "# google_openai_interface.launch(share=False, server_port=7862)  \n",
                "# ollama_interface.launch(share=False, server_port=7863) \n",
                "\n",
                "# Gradio will automatically find available ports\n",
                "# openai_interface.launch(share=False) \n",
                "# google_native_interface.launch(share=False) \n",
                "# google_openai_interface.launch(share=False)  \n",
                "ollama_interface.launch(share=False) "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "interface-sales-testing",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Interface Testing Prompts</b></h5>\n",
                "<i>Some questions to test the interfaces:</i><br>\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Sales-Related Questions (Test Core Functionality)</b></h5>\n",
                "<b>Basic Sales Inquiries:</b><br>\n",
                "1. \"What's on sale today?\"<br>\n",
                "2. \"I'm looking for something with a good discount\"<br>\n",
                "3. \"What are your best deals right now?\"<br>\n",
                "4. \"I want to buy a hat - what do you have?\"<br>\n",
                "5. \"Show me items that are 50% off\"<br><br>\n",
                "<b>Hat-Specific (60% off):</b><br>\n",
                "6. \"Do you have any winter hats?\"<br>\n",
                "7. \"I need a baseball cap for my son\"<br>\n",
                "8. \"What hat styles are popular right now?\"<br>\n",
                "9. \"Are there any hats in the sale section?\"<br>\n",
                "10. \"I'm unsure what to buy - can you help?\"<br><br>\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Edge Cases & System Message Testing</b></h5>\n",
                "<b>Belt Inquiries (Should trigger special handling):</b><br>\n",
                "11. \"Do you sell leather belts?\"<br>\n",
                "12. \"I need a belt to match my shoes\"<br>\n",
                "13. \"Where can I find belts in your store?\"<br>\n",
                "14. \"What's the price range for belts?\"<br><br>\n",
                "<b>Non-Sale Items:</b><br>\n",
                "15. \"I'm looking for shoes\"<br>\n",
                "16. \"Do you have any jackets?\"<br>\n",
                "17. \"What about pants and jeans?\"<br>\n",
                "18. \"I need a dress for a wedding\"<br><br>\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Conversation Flow Testing</b></h5>\n",
                "<b>Multi-Turn Conversations:</b><br>\n",
                "19. Start with: \"Hi, I'm just browsing\"<br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp;Follow up: \"Actually, I do need something warm for winter\"<br>\n",
                "20. \"I have a $50 budget - what can you recommend?\"<br>\n",
                "21. \"I don't like hats, what else is on sale?\"<br>\n",
                "22. \"Can you help me pick between a scarf and gloves?\"<br><br>\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Model Behavior Comparison</b></h5>\n",
                "<b>Complex Scenarios:</b><br>\n",
                "23. \"I'm shopping for my whole family - we need winter accessories\"<br>\n",
                "24. \"I'm not sure what I want, but I love a good deal\"<br>\n",
                "25. \"I hate shopping but need something quickly\"<br>\n",
                "26. \"What would you personally recommend for someone my age?\"<br><br>\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Error Handling & Edge Cases</b></h5>\n",
                "<b>Unusual Requests:</b><br>\n",
                "27. \"Do you price match other stores?\"<br>\n",
                "28. \"Can I return items if they don't fit?\"<br>\n",
                "29. \"What's your store's return policy?\"<br>\n",
                "30. \"Are you hiring? I need a job\"<br><br>\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Quick Test Sequence</b></h5>\n",
                "<b>Essential Test Set:</b><br>\n",
                "1. \"What's on sale?\"<br>\n",
                "2. \"I need a hat\"<br>\n",
                "3. \"Do you sell belts?\"<br>\n",
                "4. \"I'm not sure what to buy\"<br>\n",
                "5. \"I don't like hats - what else?\"<br>\n",
                "</div>"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "llms",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
