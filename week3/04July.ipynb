{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "8da0be76",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 13px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\">\n",
                "This notebook documents my theoretical study alongside the lab exercises conducted on <b>July 4, 2025</b>.\n",
                "</h5>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b79c8e91",
            "metadata": {},
            "source": [
                "### <u style=\"margin-bottom: 0;\">**LAB EXERCISES**</u>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "73f780ea",
            "metadata": {},
            "source": [
                "### **WEEK 3**"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8616cb70",
            "metadata": {},
            "source": [
                "#### <code>**day1.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c3956b32",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "Google Colab Notebook by <b>Edward Honner</b>:\n",
                "<a href=\"https://colab.research.google.com/drive/1DjcrYDZldAXKJ08x1uYIVCtItoLPk1Wr?usp=sharing\" target=\"_blank\">\n",
                "https://colab.research.google.com/drive/1DjcrYDZldAXKJ08x1uYIVCtItoLPk1Wr?usp=sharing</a>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "week2_lab_colab_gpu_hf",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Theory Summary</b></h5>\n",
                "<b>1. GPU Utilization in Colab</b><br>\n",
                "- Check CUDA availability via <code>torch.cuda.is_available()</code><br>\n",
                "- Run models on various GPU tiers (e.g., <b>A100</b> vs <b>T4</b> on free plan)<br>\n",
                "- Adjust Colab runtime settings to access GPU resources<br><br>\n",
                "<b>2. HuggingFace Integration</b><br>\n",
                "- Authenticate securely with HuggingFace using API tokens<br>\n",
                "- Store credentials safely using <code>secrets</code> in Colab<br>\n",
                "- Load and run pre-trained models directly from HuggingFace Hub<br><br>\n",
                "<b>3. AI Model Applications</b><br>\n",
                "- <b>Text-to-Image Generation</b>:<br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp;• <b>FLUX.1-schnell</b>: high-quality output (requires A100)<br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp;• <b>Stable Diffusion Turbo</b>: optimized for free T4 tier<br>\n",
                "- <b>Text-to-Speech</b>:<br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp;• Uses Microsoft's <b>SpeechT5</b> model for audio generation<br><br>\n",
                "<b>4. Environment Setup</b><br>\n",
                "- Install required libraries: <code>diffusers</code>, <code>transformers</code>, etc.<br>\n",
                "- Handle version compatibility across dependencies<br>\n",
                "- Adjust for varying compute needs depending on model requirements<br>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "958910c9",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Lab Exercises</b></h5>\n",
                "<b>1/</b> Rerun <code>day1_colab.ipynb</code> (the Colab Notebook by <b>Mr. Donner</b>) with <b>Python 3 & GPU T4</b><br>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "560f3a12",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prerequisites:\n",
                "%pip install -q diffusers\n",
                "%pip install -q diffusers transformers\n",
                "# %pip install -q torch\n",
                "\n",
                "# PyTorch with CUDA support:\n",
                "# For pip users:\n",
                "# For CUDA 11.8\n",
                "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
                "# OR for CUDA 12.1\n",
                "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
                "\n",
                "# For conda users:\n",
                "# For CUDA 11.8\n",
                "# !conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
                "# OR for CUDA 12.1\n",
                "# !conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
                "\n",
                "# Note: 11.8 would be the default for most users, but 12.1 would be better on newer GPUs.\n",
                "\n",
                "# Uninstalling PyTorch and related packages (in case you need to switch versions or catch some issues):\n",
                "%pip uninstall -y torch torchvision torchaudio"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b3f35f64",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<b style=\"font-size: 16px;\">Sidenote: CUDA</b><br>\n",
                "<b>CUDA (Compute Unified Device Architecture)</b> is a parallel computing platform and API developed by <b>NVIDIA</b>. It allows software developers and engineers to use a CUDA-enabled GPU for general-purpose processing - an approach known as <b>GPGPU</b> (General-Purpose computing on Graphics Processing Units).<br>\n",
                "<b>In simpler terms</b>: CUDA lets you leverage the powerful parallel processing capabilities of NVIDIA GPUs to accelerate computations that would otherwise be slow on traditional CPUs. <br>\n",
                "This is especially useful for tasks like machine learning, scientific simulations, and - as in this notebook - AI-based image generation.\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f17f58f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Libraies, packages and models\n",
                "import torch\n",
                "from diffusers.pipelines.auto_pipeline import AutoPipelineForText2Image\n",
                "from IPython.display import display\n",
                "from PIL import Image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0ae4fc71",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if CUDA is available\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "  print(\"CUDA is available! You have a GPU.\")\n",
                "else:\n",
                "  print(\"CUDA is not available. You might need to change your runtime type to include a GPU.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67cf7edc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display PyTorch and CUDA information\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"CUDA version: {torch.version.cuda}\")\n",
                "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
                "else:\n",
                "    print(\"CUDA version: N/A\")\n",
                "    print(\"GPU device: N/A\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4fb8dd44",
            "metadata": {},
            "outputs": [],
            "source": [
                "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sd-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
                "pipe.to(\"cuda\")\n",
                "\n",
                "prompt = \"A futuristic class full of students learning AI coding in a surreal style\"\n",
                "image = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8dcb90f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display the generated image  \n",
                "display(image)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "970619a5",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5406122e",
            "metadata": {},
            "source": [
                "#### <code>**day2.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "colab_link",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "Google Colab Notebook by <b>Edward Honner</b>:\n",
                "<a href=\"https://colab.research.google.com/drive/1aMaEw8A56xs0bRM4lu8z7ou18jqyybGm?usp=sharing\" target=\"_blank\">\n",
                "https://colab.research.google.com/drive/1aMaEw8A56xs0bRM4lu8z7ou18jqyybGm?usp=sharing</a>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "core_theory_concepts",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Theory Summary</b></h5>\n",
                "<b>1. HuggingFace Pipeline Architecture</b><br>\n",
                "- <b>High-Level API Design</b>: Pipelines provide a simplified interface for complex AI tasks<br>\n",
                "- <b>Two-Step Pattern</b>:<br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp;<code>my_pipeline = pipeline(\"task_name\")</code><br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp;<code>result = my_pipeline(input_data)</code><br>\n",
                "- <b>Abstraction Layer</b>: Hides model complexity while maintaining powerful functionality<br><br>\n",
                "<b>2. Training vs. Inference Distinction</b><br>\n",
                "- <b>Training</b>: Updating model weights using data to improve performance<br>\n",
                "- <b>Fine-tuning</b>: Continued training of pre-trained models<br>\n",
                "- <b>Inference</b>: Running trained models on new inputs<br>\n",
                "- <b>Pipelines</b>: Designed specifically for inference, not training<br><br>\n",
                "<b>3. Multi-Modal AI Capabilities</b><br>\n",
                "- <b>NLP Tasks</b>: Sentiment analysis, NER, Q&A, summarization, translation<br>\n",
                "- <b>Text Generation</b>: Prompt continuation<br>\n",
                "- <b>Image Generation</b>: Using Stable Diffusion for text-to-image<br>\n",
                "- <b>Audio Generation</b>: Text-to-speech with SpeechT5<br>\n",
                "- <b>Zero-shot Classification</b>: Categorizing text without specific fine-tuning<br><br>\n",
                "<b>4. GPU Computing & Resource Management</b><br>\n",
                "- <b>CUDA Integration</b>: Use <code>device=\"cuda\"</code> for GPU acceleration<br>\n",
                "- <b>Colab Runtime</b>: Understanding GPU tiers and switching environments<br>\n",
                "- <b>Optimization</b>: Minimize load time and memory usage<br><br>\n",
                "<b>5. Production Environment Considerations</b><br>\n",
                "- <b>Dependency Management</b>: Resolve pip conflicts and ensure compatibility<br>\n",
                "- <b>Authentication</b>: Secure HuggingFace API tokens via Colab secrets<br>\n",
                "- <b>Model Selection</b>: Load custom models like <code>Helsinki-NLP</code><br>\n",
                "- <b>Error Handling</b>: Differentiate real failures from misleading warnings<br><br>\n",
                "<b>6. Open Source ML Ecosystem</b><br>\n",
                "- <b>Model Hub Access</b>: Use pre-trained models via HuggingFace Hub<br>\n",
                "- <b>Community Models</b>: Examples include <code>microsoft/speecht5_tts</code>, <code>stabilityai/stable-diffusion-2</code><br>\n",
                "- <b>Task Specialization</b>: Each pipeline is optimized for a specific AI function<br>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "training_vs_inference",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em; font-size: 16px;\"><b>Sidenote 1</b></h5>\n",
                "When working with Data Science models, 2 very different activities would be carrying out: <b>Training</b> & <b>Inference</b>. <br>\n",
                "<b>1. Training</b> is the process of providing a model with data so it can learn and improve at a specific task. This involves updating its internal <b>parameters or weights</b> based on the data. If we're updating a model that was already trained, this process is known as <b>fine-tuning</b>.<br>\n",
                "<b>2. Inference</b> is a term refers to using a pre-trained model to generate outputs from new inputs. The model does not learn or change during inference — it simply applies what it has already learned. Inference is also known as <b>execution</b> or <b>running the model</b>.<br>\n",
                "<b>Examples</b>:<br>\n",
                "- All the API calls we've made to GPT, Claude, and Gemini over the past weeks are examples of inference.<br>\n",
                "- The “P” in GPT stands for <b>Pre-trained</b>, indicating that the model has already undergone extensive training.<br>\n",
                "<b>HuggingFace Pipelines</b><br>\n",
                "- The <code>pipeline</code> API in HuggingFace is designed <b>only for inference</b> — it allows us to use pre-trained models to perform tasks such as classification, translation, summarization, and more.<br><br>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "140bcff7",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Lab Exercises</b></h5>\n",
                "<b>Rerun Day 2 with Jupyter Notebook</b><br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "12256330",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8013cab5",
            "metadata": {},
            "source": [
                "#### <code>**day3.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bf56558a",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "Google Colab Notebook by <b>Edward Honner</b>:\n",
                "<a href=\"https://colab.research.google.com/drive/1WD6Y2N7ctQi1X9wa6rpkg8UfyA4iSVuz?usp=sharing\" target=\"_blank\">\n",
                "https://colab.research.google.com/drive/1WD6Y2N7ctQi1X9wa6rpkg8UfyA4iSVuz?usp=sharing</a>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "001fa878",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Lab Exercises</b></h5>\n",
                "<b>Rerun Day 3 with Jupyter Notebook<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fcd909b2",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eab21b4b",
            "metadata": {},
            "source": [
                "#### <code>**day4.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "27a978be",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "Google Colab Notebook by <b>Edward Honner</b>:\n",
                "<a href=\"https://colab.research.google.com/drive/1hhR9Z-yiqjUe7pJjVQw4c74z_V3VchLy?usp=sharing\" target=\"_blank\">\n",
                "https://colab.research.google.com/drive/1hhR9Z-yiqjUe7pJjVQw4c74z_V3VchLy?usp=sharing</a>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d5007b26",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Lab Exercises</b></h5>\n",
                "<b>Rerun Day 4 with Jupyter Notebook<br>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6b7461f4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install specific versions of PyTorch and related libraries\n",
                "\n",
                "# %pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
                "# %pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0\n",
                "\n",
                "# %pip install -q transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43e9a413",
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "from transformers.models.auto.modeling_auto import AutoModelForCausalLM\n",
                "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
                "from transformers.generation.streamers import TextStreamer\n",
                "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
                "import torch\n",
                "import gc\n",
                "import os"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "70009678",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option 1: Use environment variable\n",
                "hf_token = os.getenv('HF_TOKEN')\n",
                "if hf_token:\n",
                "\tlogin(hf_token, add_to_git_credential=True)\n",
                "else:\n",
                "\tprint(\"HF_TOKEN environment variable not found. Please set it or use manual login.\")\n",
                "\t# Uncomment the line below for manual token input\n",
                "\t# login()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4cb633d9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# instruct models\n",
                "\n",
                "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
                "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
                "GEMMA2 = \"google/gemma-2-2b-it\"\n",
                "QWEN2 = \"Qwen/Qwen2-7B-Instruct\" # exercise for you\n",
                "MIXTRAL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" # If this doesn't fit it your GPU memory, try others from the hub"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b79eebc6",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c27c3c25",
            "metadata": {},
            "source": [
                "#### <code>**day5.ipynb**</code>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eb8905a1",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "Google Colab Notebook by <b>Edward Honner</b>:\n",
                "<a href=\"https://colab.research.google.com/drive/1KSMxOCprsl1QRpt_Rq0UqCAyMtPqDQYx?usp=sharing\" target=\"_blank\">\n",
                "https://colab.research.google.com/drive/1KSMxOCprsl1QRpt_Rq0UqCAyMtPqDQYx?usp=sharing</a>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7b0182e8",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.4; margin: 0; padding: 0;\">\n",
                "<h5 style=\"margin-bottom: 0.2em;\"><b>Lab Exercises</b></h5>\n",
                "<b>Rerun Day 5 with Jupyter Notebook<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9578bf97",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "556ca7f8",
            "metadata": {},
            "source": [
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "final_report_note",
            "metadata": {},
            "source": [
                "<div style=\"font-size: 14px; line-height: 1.5; margin: 0; padding: 0;\">\n",
                "<b style=\"font-size: 20px;\">NOTE:</b> The labs from <b>Day 1</b> through <b>Day 5</b> will be documented in the final report accompanying this notebook. This includes relevant theory, implementation details, and evaluation insights where applicable.\n",
                "</div>"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "llms",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
