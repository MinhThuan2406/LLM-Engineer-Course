{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai_client = OpenAI()\n",
    "ollama_client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "system_prompt = \"\"\"You are a technical tutor and coding expert. When asked to explain code or technical concepts:\n",
    "1. Break down the code step by step\n",
    "2. Explain what each part does\n",
    "3. Explain why someone would use this pattern\n",
    "4. Provide context about when this would be useful\n",
    "5. Keep explanations clear and educational\n",
    "Respond in markdown format.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "question = \"\"\"Please explain what this code does and why: yield from {book.get(\"author\") for book in books if book.get(\"author\")}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "def get_openai_response_streaming(question):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    print(\"GPT-4o-mini Response (Streaming):\\n\")\n",
    "    \n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    response_text = \"\"\n",
    "    \n",
    "    # Get the display_id, handling the case where display_handle might be None\n",
    "    display_id = getattr(display_handle, 'display_id', None) if display_handle else None\n",
    "    \n",
    "    stream = openai_client.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response_text += chunk.choices[0].delta.content\n",
    "            if display_id:\n",
    "                update_display(Markdown(response_text), display_id=display_id)\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "openai_response = get_openai_response_streaming(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "def get_ollama_response(question):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    print(\"Llama 3.2 Response:\\n\")\n",
    "    \n",
    "    try:\n",
    "        response = ollama_client.chat.completions.create(\n",
    "            model=MODEL_LLAMA,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        response_content = response.choices[0].message.content\n",
    "        display(Markdown(response_content))\n",
    "        return response_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"**Error connecting to Ollama:** {str(e)}\\n\\n\"\n",
    "        error_msg += \"**Troubleshooting:**\\n\"\n",
    "        error_msg += \"1. Make sure Ollama is installed and running\\n\"\n",
    "        error_msg += \"2. Run `ollama serve` in a terminal\\n\"\n",
    "        error_msg += \"3. Run `ollama pull llama3.2` to download the model\\n\"\n",
    "        error_msg += \"4. Check that http://localhost:11434 is accessible\"\n",
    "        \n",
    "        display(Markdown(error_msg))\n",
    "        return None\n",
    "\n",
    "ollama_response = get_ollama_response(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
